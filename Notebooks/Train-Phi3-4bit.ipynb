{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daeb77f1",
   "metadata": {},
   "source": [
    "# Training Phi-3-mini-128k-instruct to Learn Swift Programming Language\n",
    "\n",
    "This notebook trains Microsoft's Phi-3-mini-128k-instruct model to understand and work with Swift code using a dataset of real Swift files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests accelerate peft bitsandbytes\n",
    "\n",
    "# Set PyTorch memory management environment variables to avoid fragmentation\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Explicitly set to use 2 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Define memory cleanup function\n",
    "def cleanup_memory():\n",
    "    \"\"\"Clean up GPU memory to avoid fragmentation.\"\"\"\n",
    "    print(\"Cleaning up memory...\")\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "# Define resource monitoring function\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor system and GPU resources.\"\"\"\n",
    "    # Monitor CPU and RAM\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"CPU memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Monitor GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            if hasattr(torch.cuda, 'memory_allocated'):\n",
    "                print(f\"GPU {i} ({torch.cuda.get_device_name(i)})\")\n",
    "                print(f\"  Memory allocated: {torch.cuda.memory_allocated(i) / (1024**3):.2f} GB\")\n",
    "                print(f\"  Memory reserved: {torch.cuda.memory_reserved(i) / (1024**3):.2f} GB\")\n",
    "                if hasattr(torch.cuda, 'memory_stats'):\n",
    "                    stats = torch.cuda.memory_stats(i)\n",
    "                    if 'active_bytes.all.current' in stats:\n",
    "                        print(f\"  Active memory: {stats['active_bytes.all.current'] / (1024**3):.2f} GB\")\n",
    "                    if 'reserved_bytes.all.current' in stats:\n",
    "                        print(f\"  Reserved memory: {stats['reserved_bytes.all.current'] / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fac429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure device (GPU or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    # Set up for distributed training on multiple GPUs\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Enable multi-GPU support for T4 x2\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        # For distributed training, we'll use device_map=\"auto\" when loading the model\n",
    "        print(\"Multi-GPU training enabled\")\n",
    "        \n",
    "        # Additional memory management for multi-GPU setup\n",
    "        torch.cuda.empty_cache()\n",
    "        # Set memory allocation strategy to reduce fragmentation\n",
    "        if hasattr(torch.cuda, 'memory_stats'):\n",
    "            print(\"Initial GPU memory allocated:\", torch.cuda.memory_allocated(0) / (1024**3), \"GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU - Note: Training will be much slower on CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37891d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration - using the same dataset as the original notebook\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Model configuration - using Phi-3-mini-128k-instruct\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "MAX_LENGTH = 2048  # Reduced from 4096 to save memory\n",
    "BATCH_SIZE = 1  # Reduced batch size to avoid OOM errors\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_RATIO = 0.03\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Increased to compensate for smaller batch size\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Debug mode for testing with smaller dataset\n",
    "DEBUG_MODE = True\n",
    "DEBUG_SAMPLE_SIZE = 100\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} per device\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * (2 if torch.cuda.device_count() > 1 else 1) * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"LoRA rank: {LORA_R}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ddcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    print(f\"Loading dataset: {DATASET_ID}\")\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "    \n",
    "    # If in debug mode, take a small sample of the dataset\n",
    "    if DEBUG_MODE and 'train' in data:\n",
    "        print(f\"DEBUG MODE: Sampling {DEBUG_SAMPLE_SIZE} examples from dataset\")\n",
    "        # Take a stratified sample if possible\n",
    "        data['train'] = data['train'].shuffle(seed=42).select(range(min(DEBUG_SAMPLE_SIZE, len(data['train']))))\n",
    "        print(f\"Reduced dataset size: {len(data['train'])} examples\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04376397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Phi-3 tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LENGTH)\n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_type(path):\n",
    "    \"\"\"\n",
    "    Extract the file type/category based on the file path and naming conventions in Swift projects.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path\n",
    "        \n",
    "    Returns:\n",
    "        int: The category label (0-5)\n",
    "    \"\"\"\n",
    "    path_lower = path.lower()\n",
    "    filename = path.split('/')[-1].lower()\n",
    "    \n",
    "    # Category 0: Models - Data structures and model definitions\n",
    "    if ('model' in path_lower or \n",
    "        'struct' in path_lower or \n",
    "        'entity' in path_lower or\n",
    "        'data' in path_lower and 'class' in path_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Category 1: Views - UI related files\n",
    "    elif ('view' in path_lower or \n",
    "          'ui' in path_lower or \n",
    "          'screen' in path_lower or \n",
    "          'page' in path_lower or\n",
    "          'controller' in path_lower and 'view' in path_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Category 2: Controllers - Application logic\n",
    "    elif ('controller' in path_lower or \n",
    "          'manager' in path_lower or \n",
    "          'coordinator' in path_lower or\n",
    "          'service' in path_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Category 3: Utilities - Helper functions and extensions\n",
    "    elif ('util' in path_lower or \n",
    "          'helper' in path_lower or \n",
    "          'extension' in path_lower or\n",
    "          'common' in path_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Category 4: Tests - Test files\n",
    "    elif ('test' in path_lower or \n",
    "          'spec' in path_lower or \n",
    "          'mock' in path_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Category 5: Configuration - Package and configuration files\n",
    "    elif ('package.swift' in path_lower or \n",
    "          'config' in path_lower or \n",
    "          'settings' in path_lower or\n",
    "          'info.plist' in path_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to category 3 (Utilities) if no clear category is found\n",
    "    return 3\n",
    "\n",
    "# Define category names for better readability\n",
    "category_names = {\n",
    "    0: \"Models\",\n",
    "    1: \"Views\",\n",
    "    2: \"Controllers\",\n",
    "    3: \"Utilities\",\n",
    "    4: \"Tests\",\n",
    "    5: \"Configuration\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7622d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create labels\n",
    "try:\n",
    "    # Create a new column with the extracted labels\n",
    "    labeled_data = data['train'].map(lambda example: {\n",
    "        **example,\n",
    "        'label': extract_file_type(example['path'])\n",
    "    })\n",
    "    \n",
    "    # Count the distribution of labels\n",
    "    label_counts = collections.Counter(labeled_data['label'])\n",
    "    \n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        category_name = category_names.get(label, f\"Unknown-{label}\")\n",
    "        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(label_counts.keys())\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    print(f\"\\nTotal unique labels: {num_labels}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in data preparation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "try:\n",
    "    # Shuffle the data\n",
    "    shuffled_data = labeled_data.shuffle(seed=42)\n",
    "    \n",
    "    # Split into train (80%), validation (10%), and test (10%)\n",
    "    train_size = int(0.8 * len(shuffled_data))\n",
    "    val_size = int(0.1 * len(shuffled_data))\n",
    "    \n",
    "    train_data = shuffled_data.select(range(train_size))\n",
    "    val_data = shuffled_data.select(range(train_size, train_size + val_size))\n",
    "    test_data = shuffled_data.select(range(train_size + val_size, len(shuffled_data)))\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training set label distribution: {collections.Counter(train_data['label'])}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation set label distribution: {collections.Counter(val_data['label'])}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    print(f\"Test set label distribution: {collections.Counter(test_data['label'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instruction-based prompts for the model\n",
    "def create_instruction_prompt(example):\n",
    "    \"\"\"Convert a code example into an instruction-based prompt for language learning.\"\"\"\n",
    "    code = example['content']\n",
    "    label = example['label']\n",
    "    category = category_names.get(label, f\"Unknown-{label}\")\n",
    "    \n",
    "    # Create different types of prompts to help the model learn the language\n",
    "    prompt_types = [\n",
    "        # Explain code functionality\n",
    "        \"Explain what this Swift code does and how it works:\\n\\n\",\n",
    "        \n",
    "        # Identify patterns and features\n",
    "        \"Identify and explain the key Swift language features used in this code:\\n\\n\",\n",
    "        \n",
    "        # Complete or extend code\n",
    "        \"Complete or extend this Swift code with appropriate functionality:\\n\\n\",\n",
    "        \n",
    "        # Fix or improve code\n",
    "        \"Suggest improvements or best practices for this Swift code:\\n\\n\",\n",
    "        \n",
    "        # Understand code structure\n",
    "        f\"This is a Swift {category.lower()} file. Explain its structure and purpose:\\n\\n\",\n",
    "        \n",
    "        # Code generation tasks\n",
    "        \"Write a Swift function that accomplishes the same task as this code but more efficiently:\\n\\n\",\n",
    "        \n",
    "        # Language understanding\n",
    "        \"Explain the Swift syntax and language features demonstrated in this code:\\n\\n\",\n",
    "        \n",
    "        # Learning from examples\n",
    "        \"Study this Swift code example and explain what you can learn from it:\\n\\n\"\n",
    "    ]\n",
    "    \n",
    "    # Select a random prompt type\n",
    "    instruction = random.choice(prompt_types)\n",
    "    \n",
    "    code_section = f\"```swift\\n{code}\\n```\\n\\n\"\n",
    "    \n",
    "    # Create the full prompt\n",
    "    prompt = instruction + code_section\n",
    "    \n",
    "    # Create a detailed response based on the prompt type and code category\n",
    "    if \"Explain what this Swift code does\" in instruction:\n",
    "        response = f\"This Swift code is a {category.lower()} file that \"\n",
    "        if category == \"Models\":\n",
    "            response += \"defines data structures and model objects. \"\n",
    "        elif category == \"Views\":\n",
    "            response += \"implements user interface components. \"\n",
    "        elif category == \"Controllers\":\n",
    "            response += \"manages application logic and coordinates between models and views. \"\n",
    "        elif category == \"Utilities\":\n",
    "            response += \"provides helper functions and extensions. \"\n",
    "        elif category == \"Tests\":\n",
    "            response += \"contains test cases to verify functionality. \"\n",
    "        elif category == \"Configuration\":\n",
    "            response += \"configures application settings and parameters. \"\n",
    "        \n",
    "        response += \"The code uses Swift syntax with \"\n",
    "        \n",
    "        # Add some language-specific details based on code content\n",
    "        if \"class\" in code:\n",
    "            response += \"class definitions, \"\n",
    "        if \"struct\" in code:\n",
    "            response += \"struct definitions, \"\n",
    "        if \"func\" in code:\n",
    "            response += \"function declarations, \"\n",
    "        if \"var\" in code:\n",
    "            response += \"variable declarations, \"\n",
    "        if \"let\" in code:\n",
    "            response += \"constant declarations, \"\n",
    "        if \"guard\" in code or \"if let\" in code:\n",
    "            response += \"optional unwrapping, \"\n",
    "        if \"extension\" in code:\n",
    "            response += \"extensions, \"\n",
    "        if \"protocol\" in code:\n",
    "            response += \"protocol implementations, \"\n",
    "            \n",
    "        # Remove trailing comma and space if present\n",
    "        if response.endswith(\", \"):\n",
    "            response = response[:-2] + \".\"\n",
    "        else:\n",
    "            response += \"various Swift features.\"\n",
    "    \n",
    "    elif \"Identify and explain the key Swift language features\" in instruction:\n",
    "        response = \"This Swift code demonstrates several key language features:\\n\\n\"\n",
    "        \n",
    "        # Add language features based on code content\n",
    "        features = []\n",
    "        if \"class\" in code:\n",
    "            features.append(\"1. **Classes**: Swift classes are reference types that support inheritance and reference counting.\")\n",
    "        if \"struct\" in code:\n",
    "            features.append(\"1. **Structs**: Swift structs are value types that are copied when assigned or passed as arguments.\")\n",
    "        if \"protocol\" in code:\n",
    "            features.append(\"1. **Protocols**: Similar to interfaces in other languages, protocols define a blueprint of methods, properties, and requirements.\")\n",
    "        if \"extension\" in code:\n",
    "            features.append(\"1. **Extensions**: Swift allows adding functionality to existing types through extensions.\")\n",
    "        if \"guard\" in code:\n",
    "            features.append(\"1. **Guard statements**: Used for early returns and unwrapping optionals, improving code readability.\")\n",
    "        if \"if let\" in code or \"guard let\" in code:\n",
    "            features.append(\"1. **Optional binding**: Swift's way of safely unwrapping optional values.\")\n",
    "        if \"enum\" in code:\n",
    "            features.append(\"1. **Enumerations**: Swift enums are first-class types that can have methods and computed properties.\")\n",
    "        if \"func\" in code:\n",
    "            features.append(\"1. **Functions**: Swift functions can have parameters, return values, and support closures.\")\n",
    "        \n",
    "        # If no specific features were identified, add a generic response\n",
    "        if not features:\n",
    "            features.append(\"1. **Swift syntax**: The code demonstrates standard Swift syntax and conventions.\")\n",
    "            features.append(\"2. **Type safety**: Swift's strong type system helps prevent errors at compile time.\")\n",
    "            features.append(\"3. **Readability**: Swift's clean syntax makes code easy to read and maintain.\")\n",
    "        \n",
    "        # Renumber the features\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_parts = feature.split(\": \", 1)\n",
    "            if len(feature_parts) == 2:\n",
    "                features[i] = f\"{i+1}. **{feature_parts[0].split('**')[1]}**: {feature_parts[1]}\"\n",
    "        \n",
    "        response += \"\\n\".join(features)\n",
    "    \n",
    "    elif \"Complete or extend this Swift code\" in instruction or \"Write a Swift function\" in instruction:\n",
    "        # For code generation tasks, provide a thoughtful response about how to approach the task\n",
    "        response = f\"To extend this Swift {category.lower()} code, I would consider the following approach:\\n\\n\"\n",
    "        \n",
    "        if category == \"Models\":\n",
    "            response += \"1. Add additional properties to capture more data attributes\\n\"\n",
    "            response += \"2. Implement Codable protocol for easy JSON serialization\\n\"\n",
    "            response += \"3. Add validation methods to ensure data integrity\\n\"\n",
    "            response += \"4. Include computed properties for derived values\\n\\n\"\n",
    "            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n",
    "            \n",
    "            if \"struct\" in code:\n",
    "                response += \"// Extension to add Codable conformance\\nextension MyStruct: Codable {\\n    // Codable implementation\\n}\\n\\n\"\n",
    "                response += \"// Add validation method\\nextension MyStruct {\\n    func validate() -> Bool {\\n        // Validation logic\\n        return true\\n    }\\n}\\n\"\n",
    "            else:\n",
    "                response += \"// Example extension or additional functionality\\n// that would be appropriate for this model\\n\"\n",
    "            \n",
    "            response += \"```\"\n",
    "            \n",
    "        elif category == \"Views\":\n",
    "            response += \"1. Add UI customization options\\n\"\n",
    "            response += \"2. Implement additional user interaction handlers\\n\"\n",
    "            response += \"3. Add accessibility support\\n\"\n",
    "            response += \"4. Implement view lifecycle methods\\n\\n\"\n",
    "            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n",
    "            response += \"// Example extension or additional functionality\\n// that would be appropriate for this view\\n\"\n",
    "            response += \"```\"\n",
    "            \n",
    "        else:\n",
    "            response += \"1. Add error handling to make the code more robust\\n\"\n",
    "            response += \"2. Implement additional helper methods\\n\"\n",
    "            response += \"3. Add documentation comments to improve code readability\\n\"\n",
    "            response += \"4. Consider performance optimizations where appropriate\\n\\n\"\n",
    "            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n",
    "            response += \"// Example extension or additional functionality\\n// that would be appropriate for this code\\n\"\n",
    "            response += \"```\"\n",
    "    \n",
    "    else:\n",
    "        # Generic response for other prompt types\n",
    "        response = f\"This Swift code demonstrates typical patterns used in {category.lower()} files. \"\n",
    "        response += \"It follows Swift language conventions and showcases proper syntax for defining \"\n",
    "        \n",
    "        if category == \"Models\":\n",
    "            response += \"data structures with properties and methods. Swift models typically use structs for value semantics or classes when reference semantics are needed. The code demonstrates Swift's strong typing system and property declarations.\"\n",
    "        elif category == \"Views\":\n",
    "            response += \"UI components with layout and interaction logic. Swift views often use UIKit or SwiftUI frameworks, with clear separation of UI elements and their behaviors. The code shows how Swift handles user interface components and event responses.\"\n",
    "        elif category == \"Controllers\":\n",
    "            response += \"application logic and coordination between components. Controllers in Swift manage the flow of data between models and views, implementing business logic and handling user interactions. The code demonstrates Swift's approach to application architecture.\"\n",
    "        elif category == \"Utilities\":\n",
    "            response += \"helper functions and extensions to enhance functionality. Swift utilities often leverage the language's powerful extension capabilities to add functionality to existing types. The code shows how Swift can be extended and customized through utility functions.\"\n",
    "        elif category == \"Tests\":\n",
    "            response += \"test cases with setup, execution, and verification steps. Swift tests typically use XCTest framework with arrange-act-assert pattern. The code demonstrates Swift's approach to unit testing and verification.\"\n",
    "        elif category == \"Configuration\":\n",
    "            response += \"application settings and configuration parameters. Swift configuration files often define constants, environment settings, and application parameters. The code shows how Swift handles application configuration and settings management.\"\n",
    "    \n",
    "    # Combine prompt and response for instruction tuning\n",
    "    full_text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{response}\\n\"\n",
    "    \n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"label\": label,\n",
    "        \"category\": category\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create instruction-based datasets\n",
    "try:\n",
    "    # Create instruction datasets\n",
    "    train_instructions = train_data.map(create_instruction_prompt)\n",
    "    val_instructions = val_data.map(create_instruction_prompt)\n",
    "    test_instructions = test_data.map(create_instruction_prompt)\n",
    "    \n",
    "    # Print an example to verify\n",
    "    print(\"Example instruction prompt:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(train_instructions[0]['text'])\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"Created {len(train_instructions)} training instructions\")\n",
    "    print(f\"Created {len(val_instructions)} validation instructions\")\n",
    "    print(f\"Created {len(test_instructions)} test instructions\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating instruction prompts: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Tokenize the instruction data with proper handling of padding and truncation\n",
    "def tokenize_instruction(examples):\n",
    "    \"\"\"Tokenize the instruction text with explicit padding and truncation settings.\"\"\"\n",
    "    # Process one example at a time to avoid dimension issues\n",
    "    results = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    \n",
    "    for text in examples['text']:\n",
    "        # Tokenize with explicit padding and truncation settings\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=None  # Return Python lists, not PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Add to results\n",
    "        results[\"input_ids\"].append(encoded[\"input_ids\"])\n",
    "        results[\"attention_mask\"].append(encoded[\"attention_mask\"])\n",
    "        results[\"labels\"].append(encoded[\"input_ids\"].copy())  # Copy input_ids for labels\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb007c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Apply tokenization to each split\n",
    "    tokenized_train = train_instructions.map(\n",
    "        tokenize_instruction,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content', 'label', 'text', 'prompt', 'response', 'category']\n",
    "    )\n",
    "    \n",
    "    tokenized_val = val_instructions.map(\n",
    "        tokenize_instruction,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content', 'label', 'text', 'prompt', 'response', 'category']\n",
    "    )\n",
    "    \n",
    "    # Set the format for PyTorch\n",
    "    tokenized_train.set_format(\"torch\")\n",
    "    tokenized_val.set_format(\"torch\")\n",
    "    \n",
    "    print(f\"Tokenized {len(tokenized_train)} training examples\")\n",
    "    print(f\"Tokenized {len(tokenized_val)} validation examples\")\n",
    "    print(\"Data tokenization complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments with optimized settings for multi-GPU training\n",
    "try:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(\"./phi3_swift_model\", exist_ok=True)\n",
    "    \n",
    "    # Configure training arguments with distributed training settings for GPU/CPU\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./phi3_swift_model\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,  # Use mixed precision training\n",
    "        gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "        # Distributed training parameters\n",
    "        local_rank=int(os.environ.get(\"LOCAL_RANK\", -1)),  # For distributed training\n",
    "        ddp_find_unused_parameters=False,  # Optimize DDP\n",
    "        dataloader_num_workers=2,  # Reduced from 4 to save memory\n",
    "        dataloader_pin_memory=False,  # Disable pin memory to reduce memory usage\n",
    "        report_to=\"none\"  # Disable reporting to avoid extra overhead\n",
    "    )\n",
    "    \n",
    "    print(f\"Training arguments configured for {'multi-GPU' if torch.cuda.device_count() > 1 else 'single-GPU'} training\")\n",
    "    print(f\"Using gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
    "    print(f\"Using mixed precision: {training_args.fp16}\")\n",
    "    print(f\"Local rank: {training_args.local_rank}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error setting up training arguments: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7521e76",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Load and prepare the model\n",
    "try:\n",
    "    print(f\"Loading {MODEL_NAME} with 4-bit quantization...\")\n",
    "    \n",
    "    # Clean up memory before model loading\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Configure 4-bit quantization with memory-efficient settings\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,  # Use nested quantization for more memory efficiency\n",
    "        bnb_4bit_quant_type=\"nf4\",        # Normalized float 4 for better accuracy\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        llm_int8_has_fp16_weight=False,   # Reduce memory footprint\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_skip_modules=None,\n",
    "    )\n",
    "    \n",
    "    # Load model with proper device mapping for multi-GPU distribution\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,  # Automatically distribute across available GPUs\n",
    "        offload_folder=\"offload\",  # Enable CPU offloading if needed\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False,  # Disable KV cache during training for better memory efficiency\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully loaded model with 4-bit quantization\")\n",
    "    \n",
    "    # Configure LoRA for efficient fine-tuning\n",
    "    print(\"Setting up LoRA fine-tuning...\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    \n",
    "    # Prepare the model for training - crucial for memory efficiency\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print information about the quantized model\n",
    "    print(f\"Model loaded and configured with 4-bit quantization and LoRA (rank={LORA_R})\")\n",
    "    print(f\"Model architecture: {model.__class__.__name__}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n",
    "    \n",
    "    # Monitor GPU memory after model loading\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            print(f\"GPU {i} memory after model loading: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Create a custom data collator that properly handles the data\n",
    "class CustomDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features):\n",
    "        # Ensure all features have the same keys\n",
    "        if not all(k in features[0] for k in [\"input_ids\", \"attention_mask\", \"labels\"]):\n",
    "            raise ValueError(\"Some features are missing required keys\")\n",
    "        \n",
    "        # Create a batch with proper padding\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "            \"labels\": torch.stack([f[\"labels\"] for f in features])\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Create data collator for language modeling\n",
    "data_collator = CustomDataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal language modeling, not masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Verify model device\n",
    "model_device = next(model.parameters()).device\n",
    "device_type = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Model is on {device_type} device: {model_device}\")\n",
    "\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor system resources during training with detailed GPU memory tracking\n",
    "def monitor_resources():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    mem = psutil.virtual_memory()\n",
    "    cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "    \n",
    "    print(f\"\\nSystem Resources:\")\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")\n",
    "    print(f\"Process Memory: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"System Memory: {mem.percent}% used, {mem.available / 1024 / 1024:.2f} MB available\")\n",
    "    \n",
    "    # Add detailed GPU memory tracking\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nGPU Memory Usage ({num_gpus} GPUs detected):\")\n",
    "        \n",
    "        total_allocated = 0\n",
    "        total_reserved = 0\n",
    "        total_free = 0\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            properties = torch.cuda.get_device_properties(i)\n",
    "            total_memory = properties.total_memory / (1024**3)\n",
    "            free_memory = total_memory - allocated\n",
    "            \n",
    "            total_allocated += allocated\n",
    "            total_reserved += reserved\n",
    "            total_free += free_memory\n",
    "            \n",
    "            print(f\"  GPU {i} ({properties.name}):\")\n",
    "            print(f\"    Total memory: {total_memory:.2f} GB\")\n",
    "            print(f\"    Allocated: {allocated:.2f} GB ({allocated/total_memory*100:.1f}%)\")\n",
    "            print(f\"    Reserved: {reserved:.2f} GB ({reserved/total_memory*100:.1f}%)\")\n",
    "            print(f\"    Free: {free_memory:.2f} GB ({free_memory/total_memory*100:.1f}%)\")\n",
    "            \n",
    "            # Show detailed memory statistics if available\n",
    "            if hasattr(torch.cuda, 'memory_stats'):\n",
    "                stats = torch.cuda.memory_stats(i)\n",
    "                if 'active_bytes.all.current' in stats:\n",
    "                    active = stats['active_bytes.all.current'] / (1024**3)\n",
    "                    print(f\"    Active memory: {active:.2f} GB\")\n",
    "                if 'inactive_split_bytes.all.current' in stats:\n",
    "                    inactive = stats['inactive_split_bytes.all.current'] / (1024**3)\n",
    "                    print(f\"    Inactive split: {inactive:.2f} GB\")\n",
    "                if 'reserved_bytes.all.current' in stats:\n",
    "                    reserved_bytes = stats['reserved_bytes.all.current'] / (1024**3)\n",
    "                    print(f\"    Reserved bytes: {reserved_bytes:.2f} GB\")\n",
    "                    \n",
    "        print(f\"\\n  Total across all GPUs:\")\n",
    "        print(f\"    Allocated: {total_allocated:.2f} GB\")\n",
    "        print(f\"    Reserved: {total_reserved:.2f} GB\")\n",
    "        print(f\"    Free: {total_free:.2f} GB\")\n",
    "        \n",
    "        # Check for potential OOM conditions\n",
    "        if any(torch.cuda.memory_allocated(i) / torch.cuda.get_device_properties(i).total_memory > 0.90 for i in range(num_gpus)):\n",
    "            print(\"\\n  âš ï¸ WARNING: At least one GPU is using >90% of available memory - OOM risk is high!\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd718f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom training loop with enhanced memory management for multi-GPU setup\n",
    "try:\n",
    "    print(\"Starting training with enhanced memory management...\")\n",
    "    \n",
    "    # Monitor resources before training\n",
    "    print(\"Resources before training:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Additional memory cleanup before training\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Set PyTorch to optimize for multi-GPU training\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Configuring PyTorch for {torch.cuda.device_count()} GPUs...\")\n",
    "        \n",
    "        # Enable TF32 precision for faster training (on Ampere GPUs)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        \n",
    "        # Disable memory-intensive CUDA features\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        # Lower memory allocation to avoid OOM\n",
    "        torch.cuda.set_per_process_memory_fraction(0.80)  # Further reduced to prevent OOM\n",
    "        \n",
    "        # Create explicit model shard strategy for multi-GPU\n",
    "        print(\"Model is distributed across GPUs with device_map='auto'\")\n",
    "        \n",
    "        # Override trainer's distributed strategy for better GPU utilization\n",
    "        training_args.ddp_find_unused_parameters = True\n",
    "    \n",
    "    # Create a custom training loop with extra OOM prevention\n",
    "    class OOMGuardCallback(transformers.TrainerCallback):\n",
    "        \"\"\"Custom callback that detects and prevents OOM errors\"\"\"\n",
    "        def on_step_end(self, args, state, control, **kwargs):\n",
    "            # Check memory usage on each GPU after step\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                allocated = torch.cuda.memory_allocated(i) / torch.cuda.get_device_properties(i).total_memory\n",
    "                if allocated > 0.92:  # Over 92% memory usage\n",
    "                    # Force immediate garbage collection and cache clearing\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(f\"\\nâš ï¸ Warning: GPU {i} memory usage high ({allocated*100:.1f}%). Forcing cache clear.\")\n",
    "                    # Pause briefly to allow memory release\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "        def on_epoch_end(self, args, state, control, **kwargs):\n",
    "            # Clean up between epochs for better stability\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\\nCompleted epoch - clearing memory cache\")\n",
    "    \n",
    "    # Add our OOM prevention callback\n",
    "    trainer.add_callback(OOMGuardCallback())\n",
    "    \n",
    "    # Start training with a timeout and checkpointing\n",
    "    max_training_time = 6 * 60 * 60  # 6 hours max\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run training with OOM handling\n",
    "    try:\n",
    "        # First try with normal parameters\n",
    "        print(\"\\nStarting training with regular settings...\")\n",
    "        train_result = trainer.train()\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            # If we hit OOM, try recovery steps\n",
    "            print(\"\\nðŸ›‘ CUDA OOM detected. Attempting recovery...\")\n",
    "            \n",
    "            # Clear all GPU memory\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats(i)\n",
    "            \n",
    "            # Try reducing sequence length further if needed\n",
    "            if MAX_LENGTH > 1024:\n",
    "                old_max_len = MAX_LENGTH\n",
    "                MAX_LENGTH = 1024\n",
    "                print(f\"Reducing sequence length from {old_max_len} to {MAX_LENGTH}\")\n",
    "                \n",
    "                # Reload tokenizer with new max length\n",
    "                tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LENGTH)\n",
    "                if tokenizer.pad_token is None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "                # We need to retokenize the data, but for now just use what we have\n",
    "                # and set the training args to handle the shorter sequence length\n",
    "                training_args.max_length = MAX_LENGTH\n",
    "            \n",
    "            # Try with even smaller batch size and more gradient accumulation\n",
    "            training_args.per_device_train_batch_size = 1\n",
    "            training_args.gradient_accumulation_steps = 16\n",
    "            print(\"Reduced batch size to 1 and increased gradient accumulation to 16\")\n",
    "            \n",
    "            # Re-create trainer with updated settings\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_train,\n",
    "                eval_dataset=tokenized_val,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                callbacks=[early_stopping_callback, OOMGuardCallback()]\n",
    "            )\n",
    "            \n",
    "            # Try again with new settings\n",
    "            print(\"\\nRetrying training with reduced memory settings...\")\n",
    "            train_result = trainer.train()\n",
    "    \n",
    "    # Monitor resources after training\n",
    "    print(\"\\nResources after training:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Print training results\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {elapsed_time/60:.2f} minutes\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"\\nSaving model...\")\n",
    "    trainer.save_model(\"./phi3_swift_model\")\n",
    "    \n",
    "    # Save adapter separately for easier loading\n",
    "    if hasattr(model, \"save_pretrained\"):\n",
    "        try:\n",
    "            model.save_pretrained(\"./phi3_swift_model_adapter\")\n",
    "            print(\"Saved LoRA adapter to ./phi3_swift_model_adapter\")\n",
    "        except Exception as save_error:\n",
    "            print(f\"Error saving adapter: {save_error}\")\n",
    "    \n",
    "    print(\"Model saved successfully\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    cleanup_memory()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error during training: {e}\")\n",
    "    \n",
    "    # Print stack trace for debugging\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Try to save checkpoint if possible\n",
    "    try:\n",
    "        print(\"\\nAttempting to save checkpoint after error...\")\n",
    "        trainer.save_model(\"./phi3_swift_model_checkpoint_after_error\")\n",
    "        print(\"Emergency checkpoint saved to ./phi3_swift_model_checkpoint_after_error\")\n",
    "    except:\n",
    "        print(\"Could not save emergency checkpoint\")\n",
    "    \n",
    "    # Monitor resources after error\n",
    "    print(\"Resources after error:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70854fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with Swift code examples\n",
    "try:\n",
    "    print(\"Testing the model with Swift code examples...\")\n",
    "    \n",
    "    # Function to generate responses for test examples\n",
    "    def generate_response(prompt):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract just the assistant's response\n",
    "        if \"<|assistant|>\" in response:\n",
    "            response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "        return response\n",
    "    \n",
    "    # Test prompts for different Swift language tasks\n",
    "    test_prompts = [\n",
    "        # Explain Swift syntax\n",
    "        \"<|user|>\\nExplain the key features of Swift's optional unwrapping syntax:\\n\\n```swift\\nfunc processName(_ name: String?) {\\n    guard let unwrappedName = name else {\\n        print(\\\"No name provided\\\")\\n        return\\n    }\\n    print(\\\"Hello, \\\\(unwrappedName)!\\\")\\n}\\n```\\n<|assistant|>\",\n",
    "        \n",
    "        # Code completion\n",
    "        \"<|user|>\\nComplete this Swift function that calculates the factorial of a number:\\n\\n```swift\\nfunc factorial(_ n: Int) -> Int {\\n    // Add implementation here\\n}\\n```\\n<|assistant|>\",\n",
    "        \n",
    "        # Debugging help\n",
    "        \"<|user|>\\nWhat's wrong with this Swift code and how can I fix it?\\n\\n```swift\\nclass Person {\\n    var name: String\\n    var age: Int\\n    \\n    func greet() {\\n        print(\\\"Hello, my name is \\\\(name) and I am \\\\(age) years old.\\\")\\n    }\\n}\\n\\nlet person = Person()\\nperson.greet()\\n```\\n<|assistant|>\",\n",
    "        \n",
    "        # Swift best practices\n",
    "        \"<|user|>\\nExplain Swift best practices for error handling:\\n<|assistant|>\"\n",
    "    ]\n",
    "    \n",
    "    # Generate and print responses\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\nTest {i+1}:\\n{'-'*40}\")\n",
    "        print(f\"Prompt: {prompt.split('<|assistant|>')[0].replace('<|user|>', '')}\")\n",
    "        response = generate_response(prompt)\n",
    "        print(f\"\\nResponse:\\n{response}\\n\")\n",
    "    \n",
    "    print(\"\\nTesting complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
