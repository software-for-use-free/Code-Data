{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4835b944",
   "metadata": {},
   "source": [
    "# Training Phi-3-mini-128k-instruct to Learn Swift Programming Language\n",
    "\n",
    "This notebook trains Microsoft's Phi-3-mini-128k-instruct model to understand and work with Swift code using a dataset of real Swift files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db01a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTION TRACKING SYSTEM\n",
    "# This cell must be executed first\n",
    "\n",
    "# Create execution tracker\n",
    "EXECUTION_STATUS = {\n",
    "    \"setup_complete\": False,\n",
    "    \"data_loaded\": False,\n",
    "    \"model_initialized\": False,\n",
    "    \"trainer_created\": False,\n",
    "    \"training_complete\": False,\n",
    "    \"testing_complete\": False\n",
    "}\n",
    "\n",
    "def update_status(stage):\n",
    "    \"\"\"Update execution status and print progress\"\"\"\n",
    "    global EXECUTION_STATUS\n",
    "    EXECUTION_STATUS[stage] = True\n",
    "    \n",
    "    # Calculate progress\n",
    "    completed = sum(1 for status in EXECUTION_STATUS.values() if status)\n",
    "    total = len(EXECUTION_STATUS)\n",
    "    progress = completed / total * 100\n",
    "    \n",
    "    print(f\"‚úì {stage.replace('_', ' ').title()} - Progress: {progress:.1f}%\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Flag to begin execution\n",
    "print(\"Starting Phi-3 training pipeline...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50479eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 1: SETUP - Install required libraries and configure environment\n",
    "print(\"üì¶ Installing required libraries...\")\n",
    "\n",
    "# First, install bitsandbytes with multi-backend support for CPU offloading during training\n",
    "print(\"üëâ Installing BitsAndBytes with multi-backend support for CPU offloading...\")\n",
    "!pip uninstall -y bitsandbytes  # Remove any existing installation\n",
    "!pip install git+https://github.com/TimDettmers/bitsandbytes.git  # Install latest version\n",
    "!pip install --upgrade transformers datasets evaluate torch scikit-learn tqdm dropbox requests accelerate peft\n",
    "\n",
    "# Verify bitsandbytes installation for training with offloading\n",
    "print(\"üëâ Verifying BitsAndBytes installation...\")\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    import torch\n",
    "    print(f\"‚úì BitsAndBytes version: {bnb.__version__}\")\n",
    "    \n",
    "    # Check if compiled with CUDA support (different ways depending on version)\n",
    "    cuda_available = False\n",
    "    \n",
    "    # Method 1: Check COMPILED_WITH_CUDA attribute (older versions)\n",
    "    if hasattr(bnb, \"COMPILED_WITH_CUDA\"):\n",
    "        cuda_available = bnb.COMPILED_WITH_CUDA\n",
    "        print(f\"‚úì Compiled with CUDA: {cuda_available}\")\n",
    "    \n",
    "    # Method 2: Check cuda_specs module (newer versions)\n",
    "    elif hasattr(bnb, \"cuda_specs\") and hasattr(bnb.cuda_specs, \"CUDA_AVAILABLE\"):\n",
    "        cuda_available = bnb.cuda_specs.CUDA_AVAILABLE\n",
    "        print(f\"‚úì CUDA available (from cuda_specs): {cuda_available}\")\n",
    "    \n",
    "    # Method 3: Check if CUDA is available through torch\n",
    "    elif hasattr(torch, \"cuda\") and torch.cuda.is_available():\n",
    "        cuda_available = True\n",
    "        print(f\"‚úì CUDA available through PyTorch: {torch.cuda.is_available()}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CUDA support not detected in bitsandbytes\")\n",
    "    \n",
    "    # Check for multi-backend support (different ways depending on version)\n",
    "    has_multi_backend = False\n",
    "    \n",
    "    # Method 1: Check for get_available_modules function\n",
    "    if \"get_available_modules\" in dir(bnb):\n",
    "        has_multi_backend = True\n",
    "        print(\"‚úì Multi-backend support detected (get_available_modules)\")\n",
    "    \n",
    "    # Method 2: Check for cuda module with has_cuda_extension\n",
    "    elif hasattr(bnb, \"cuda\"):\n",
    "        # Only check for has_cuda_extension if bnb.cuda exists\n",
    "        if hasattr(bnb.cuda, \"has_cuda_extension\"):\n",
    "            has_multi_backend = True\n",
    "            print(\"‚úì Multi-backend support detected (cuda.has_cuda_extension)\")\n",
    "    \n",
    "    # Method 3: Check for diagnostics.cuda module\n",
    "    elif hasattr(bnb, \"diagnostics\") and hasattr(bnb.diagnostics, \"cuda\"):\n",
    "        print(\"‚úì CUDA diagnostics module detected\")\n",
    "        has_multi_backend = True\n",
    "    \n",
    "    if not has_multi_backend:\n",
    "        print(\"‚ö†Ô∏è Multi-backend support not detected, but continuing anyway\")\n",
    "    \n",
    "    print(\"BitsAndBytes installation looks good enough to proceed!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è BitsAndBytes import failed: {e}\")\n",
    "    print(\"Installing fallback version...\")\n",
    "    !pip install bitsandbytes\n",
    "\n",
    "# Set PyTorch memory management environment variables to avoid fragmentation and OOM issues\n",
    "import os\n",
    "# Configure CUDA memory allocation for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Explicitly set to use 2 GPUs\n",
    "# Enable better memory handling with CPU offloading\n",
    "os.environ[\"ACCELERATE_USE_CPU_OFFLOAD\"] = \"1\"\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"fp16\"\n",
    "# Enable disk offloading if needed\n",
    "os.environ[\"ACCELERATE_ENABLE_DISK_OFFLOAD\"] = \"1\"\n",
    "# Enable training with CPU offloading\n",
    "os.environ[\"BNB_OFFLOAD_TRAINING\"] = \"1\"  # Critical for training with CPU offload\n",
    "\n",
    "# Update execution status\n",
    "if 'update_status' in globals():\n",
    "    update_status(\"setup_complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9908c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 1 (cont): Import required libraries\n",
    "print(\"üìö Importing libraries and setting up environment...\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set environment variables to disable unnecessary features\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "# Skip unnecessary imports in transformers\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"  # Temporarily set offline mode\n",
    "os.environ[\"TRANSFORMERS_SKIP_TORCH_VISION_IMPORT\"] = \"1\"  # Skip image-related components\n",
    "\n",
    "# Import transformers components individually to avoid problematic dependencies\n",
    "from transformers.models.auto.modeling_auto import AutoModelForCausalLM\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "# Reset offline mode after imports\n",
    "os.environ.pop(\"TRANSFORMERS_OFFLINE\", None)\n",
    "\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "# Import GGUF for model quantization\n",
    "try:\n",
    "    try:\n",
    "        import ctransformers\n",
    "    except ImportError:\n",
    "        print(\"ctransformers package not found. Installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([\"pip\", \"install\", \"-U\", \"ctransformers\", \"--no-cache-dir\"])\n",
    "        import ctransformers\n",
    "\n",
    "    # Also install llama-cpp-python for additional GGUF compatibility\n",
    "    try:\n",
    "        import llama_cpp\n",
    "    except ImportError:\n",
    "        print(\"llama-cpp-python package not found. Installing...\")\n",
    "        subprocess.check_call([\"pip\", \"install\", \"-U\", \"llama-cpp-python\", \"--no-cache-dir\"])\n",
    "        import llama_cpp\n",
    "    \n",
    "    print(\"GGUF libraries imported successfully - ctransformers version:\", \n",
    "          ctransformers.__version__ if hasattr(ctransformers, \"__version__\") else \"unknown\")\n",
    "    print(\"llama-cpp-python version:\", \n",
    "          llama_cpp.__version__ if hasattr(llama_cpp, \"__version__\") else \"unknown\")\n",
    "except Exception as e:\n",
    "    print(f\"Error importing GGUF libraries: {e}\")\n",
    "    print(\"Will fallback to 4-bit quantization using BitsAndBytes\")\n",
    "\n",
    "# Focus on language model training without unnecessary dependencies\n",
    "\n",
    "# Define Kaggle-optimized memory cleanup function\n",
    "def cleanup_memory():\n",
    "    \"\"\"\n",
    "    Clean up GPU memory with aggressive management optimized for Kaggle environments.\n",
    "    Handles GPU memory, system temp files, and Python memory with multiple strategies.\n",
    "    \"\"\"\n",
    "    print(\"Cleaning up memory with Kaggle-optimized strategies...\")\n",
    "    \n",
    "    # Clear Python's garbage collector multiple times\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "    \n",
    "    # Force Python to release memory to OS if possible\n",
    "    if hasattr(gc, 'mem_free'):\n",
    "        gc.mem_free()  # Some Python installations support this\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # Empty CUDA cache with multiple strategies\n",
    "        torch.cuda.empty_cache()  # Standard cleanup\n",
    "        torch.cuda.synchronize()  # Wait for all CUDA operations to finish\n",
    "        \n",
    "        # Try to release all CUDA memory and reinitialize if necessary\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            # Force deallocate any unused memory\n",
    "            if hasattr(torch.cuda, 'reset_peak_memory_stats'):\n",
    "                torch.cuda.reset_peak_memory_stats(i)\n",
    "            if hasattr(torch.cuda, 'reset_accumulated_memory_stats'):\n",
    "                try:\n",
    "                    torch.cuda.reset_accumulated_memory_stats(i)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Print memory info after cleanup\n",
    "            if hasattr(torch.cuda, 'memory_allocated'):\n",
    "                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "                reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "                print(f\"  GPU {i} after cleanup: {alloc:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "        \n",
    "        # Try to explicitly clear CUDA memory pools\n",
    "        try:\n",
    "            # This works on newer PyTorch versions\n",
    "            torch.cuda._cached_memory_pool.empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Explicitly delete any large objects that might be in memory\n",
    "    # First identify all large objects\n",
    "    large_objects = []\n",
    "    \n",
    "    # Look for larger threshold on tensors and arrays which are more common in ML\n",
    "    for var_name in list(globals().keys()):\n",
    "        var = globals()[var_name]\n",
    "        try:\n",
    "            # Calculate size more accurately for common ML objects\n",
    "            if isinstance(var, torch.Tensor):\n",
    "                size = var.element_size() * var.nelement()\n",
    "                if size > 1e6:  # 1MB\n",
    "                    large_objects.append(var_name)\n",
    "            elif isinstance(var, (list, dict, set)):\n",
    "                size = sys.getsizeof(var)\n",
    "                if size > 5e6:  # 5MB\n",
    "                    large_objects.append(var_name)\n",
    "            # Look for numpy arrays too\n",
    "            elif 'numpy' in str(type(var)):\n",
    "                try:\n",
    "                    size = var.nbytes\n",
    "                    if size > 1e6:\n",
    "                        large_objects.append(var_name)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Delete identified large objects\n",
    "    for obj in large_objects:\n",
    "        if obj in globals():\n",
    "            print(f\"  Deleting large object: {obj}\")\n",
    "            try:\n",
    "                del globals()[obj]\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Clean Kaggle temp directories if they exist\n",
    "    try:\n",
    "        # Common Kaggle temp directories that might accumulate files\n",
    "        kaggle_temp_dirs = [\n",
    "            \"/tmp/transformers_cache\",\n",
    "            \"/tmp/torch_cache\",\n",
    "            \"/tmp/huggingface\",\n",
    "            \"/kaggle/working/tmp\",\n",
    "            \"/kaggle/temp\"\n",
    "        ]\n",
    "        \n",
    "        import os\n",
    "        import shutil\n",
    "        \n",
    "        for temp_dir in kaggle_temp_dirs:\n",
    "            if os.path.exists(temp_dir) and os.path.isdir(temp_dir):\n",
    "                print(f\"Cleaning Kaggle temp directory: {temp_dir}\")\n",
    "                try:\n",
    "                    # Delete files older than 1 hour\n",
    "                    for root, dirs, files in os.walk(temp_dir):\n",
    "                        for f in files:\n",
    "                            try:\n",
    "                                full_path = os.path.join(root, f)\n",
    "                                # Only delete if older than 1 hour and not a necessary file\n",
    "                                if os.path.getmtime(full_path) < time.time() - 3600:\n",
    "                                    os.remove(full_path)\n",
    "                            except:\n",
    "                                pass\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cleaning temp dir {temp_dir}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Kaggle temp cleanup: {e}\")\n",
    "    \n",
    "    # Run gc again at the end\n",
    "    gc.collect()\n",
    "    \n",
    "    # Try to force system to release memory to OS\n",
    "    try:\n",
    "        # Some systems support this call to release memory to the OS\n",
    "        import ctypes\n",
    "        libc = ctypes.CDLL(\"libc.so.6\")\n",
    "        libc.malloc_trim(0)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "# Define resource monitoring function\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor system and GPU resources.\"\"\"\n",
    "    # Monitor CPU and RAM\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"CPU memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Monitor GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs: {num_gpus}\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            if hasattr(torch.cuda, 'memory_allocated'):\n",
    "                print(f\"GPU {i} ({torch.cuda.get_device_name(i)})\")\n",
    "                print(f\"  Memory allocated: {torch.cuda.memory_allocated(i) / (1024**3):.2f} GB\")\n",
    "                print(f\"  Memory reserved: {torch.cuda.memory_reserved(i) / (1024**3):.2f} GB\")\n",
    "                if hasattr(torch.cuda, 'memory_stats'):\n",
    "                    stats = torch.cuda.memory_stats(i)\n",
    "                    if 'active_bytes.all.current' in stats:\n",
    "                        print(f\"  Active memory: {stats['active_bytes.all.current'] / (1024**3):.2f} GB\")\n",
    "                    if 'reserved_bytes.all.current' in stats:\n",
    "                        print(f\"  Reserved memory: {stats['reserved_bytes.all.current'] / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c38706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure device (GPU or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    # Set up for distributed training on multiple GPUs\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Enable multi-GPU support for T4 x2\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        # For distributed training, we'll use device_map=\"auto\" when loading the model\n",
    "        print(\"Multi-GPU training enabled\")\n",
    "        \n",
    "        # Additional memory management for multi-GPU setup\n",
    "        torch.cuda.empty_cache()\n",
    "        # Set memory allocation strategy to reduce fragmentation\n",
    "        if hasattr(torch.cuda, 'memory_stats'):\n",
    "            print(\"Initial GPU memory allocated:\", torch.cuda.memory_allocated(0) / (1024**3), \"GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU - Note: Training will be much slower on CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b4be0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Dataset configuration - using the same dataset as the original notebook\n",
    "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
    "\n",
    "# Model configuration - using Phi-3-mini-128k-instruct\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "MAX_LENGTH = 4096  # Phi-3 can handle long sequences natively\n",
    "BATCH_SIZE = 1  # Reduced batch size to 1 to minimize memory usage per GPU\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_RATIO = 0.03\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Increased gradient accumulation to compensate for smaller batch size\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_DIR = \"./phi3_swift_model\"  # Single output directory for all model artifacts\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 8  # Reduced from 16 to save memory\n",
    "LORA_ALPHA = 16  # Reduced from 32 to save memory\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Debug mode for testing with smaller dataset\n",
    "DEBUG_MODE = True\n",
    "DEBUG_SAMPLE_SIZE = 100\n",
    "\n",
    "# Memory optimization flags\n",
    "USE_CPU_OFFLOAD = True\n",
    "USE_MEMORY_EFFICIENT_ATTENTION = True\n",
    "USE_ACTIVATION_CHECKPOINTING = True\n",
    "USE_SEQUENTIAL_OFFLOAD = True\n",
    "OFFLOAD_FOLDER = \"./offload_folder\"  # For disk offloading\n",
    "os.makedirs(OFFLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} per device\")\n",
    "print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * (2 if torch.cuda.device_count() > 1 else 1) * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"LoRA rank: {LORA_R}\")\n",
    "print(f\"Memory optimizations: CPU Offload={USE_CPU_OFFLOAD}, Memory-Efficient Attention={USE_MEMORY_EFFICIENT_ATTENTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 2: DATA PREPARATION - Load and prepare the dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: DATASET PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"üìä Loading and preparing the dataset...\")\n",
    "\n",
    "# Function to load dataset with retry logic\n",
    "def load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load a dataset with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n",
    "            data = load_dataset(dataset_id, trust_remote_code=True)\n",
    "            print(f\"‚úì Dataset loaded successfully with {len(data['train'])} examples\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"‚ùå Maximum retries reached. Could not load dataset.\")\n",
    "                raise\n",
    "\n",
    "# Load the dataset with retry logic\n",
    "try:\n",
    "    print(f\"üì• Loading dataset: {DATASET_ID}\")\n",
    "    data = load_dataset_with_retry(DATASET_ID)\n",
    "    print(\"Dataset structure:\")\n",
    "    print(data)\n",
    "    \n",
    "    # If in debug mode, take a small sample of the dataset\n",
    "    if DEBUG_MODE and 'train' in data:\n",
    "        print(f\"üîç DEBUG MODE: Sampling {DEBUG_SAMPLE_SIZE} examples from dataset\")\n",
    "        # Take a stratified sample if possible\n",
    "        data['train'] = data['train'].shuffle(seed=42).select(range(min(DEBUG_SAMPLE_SIZE, len(data['train']))))\n",
    "        print(f\"‚úì Reduced dataset size: {len(data['train'])} examples\")\n",
    "    \n",
    "    # Update execution status\n",
    "    if 'update_status' in globals():\n",
    "        update_status(\"data_loaded\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fatal error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f924ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and column names\n",
    "def verify_dataset_structure(dataset):\n",
    "    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n",
    "    required_columns = ['repo_name', 'path', 'content']\n",
    "    if 'train' not in dataset:\n",
    "        print(\"WARNING: Dataset does not have a 'train' split.\")\n",
    "        return False\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n",
    "    if missing_columns:\n",
    "        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Dataset structure verification passed.\")\n",
    "    return True\n",
    "\n",
    "# Verify dataset structure\n",
    "dataset_valid = verify_dataset_structure(data)\n",
    "if not dataset_valid:\n",
    "    print(\"Dataset structure is not as expected. Proceeding with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Phi-3 tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LENGTH)\n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a799533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_type(path):\n",
    "    \"\"\"\n",
    "    Extract the file type/category based on the file path and naming conventions in Swift projects.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path\n",
    "        \n",
    "    Returns:\n",
    "        int: The category label (0-5)\n",
    "    \"\"\"\n",
    "    path_lower = path.lower()\n",
    "    filename = path.split('/')[-1].lower()\n",
    "    \n",
    "    # Category 0: Models - Data structures and model definitions\n",
    "    if ('model' in path_lower or \n",
    "        'struct' in path_lower or \n",
    "        'entity' in path_lower or\n",
    "        'data' in path_lower and 'class' in path_lower):\n",
    "        return 0\n",
    "    \n",
    "    # Category 1: Views - UI related files\n",
    "    elif ('view' in path_lower or \n",
    "          'ui' in path_lower or \n",
    "          'screen' in path_lower or \n",
    "          'page' in path_lower or\n",
    "          'controller' in path_lower and 'view' in path_lower):\n",
    "        return 1\n",
    "    \n",
    "    # Category 2: Controllers - Application logic\n",
    "    elif ('controller' in path_lower or \n",
    "          'manager' in path_lower or \n",
    "          'coordinator' in path_lower or\n",
    "          'service' in path_lower):\n",
    "        return 2\n",
    "    \n",
    "    # Category 3: Utilities - Helper functions and extensions\n",
    "    elif ('util' in path_lower or \n",
    "          'helper' in path_lower or \n",
    "          'extension' in path_lower or\n",
    "          'common' in path_lower):\n",
    "        return 3\n",
    "    \n",
    "    # Category 4: Tests - Test files\n",
    "    elif ('test' in path_lower or \n",
    "          'spec' in path_lower or \n",
    "          'mock' in path_lower):\n",
    "        return 4\n",
    "    \n",
    "    # Category 5: Configuration - Package and configuration files\n",
    "    elif ('package.swift' in path_lower or \n",
    "          'config' in path_lower or \n",
    "          'settings' in path_lower or\n",
    "          'info.plist' in path_lower):\n",
    "        return 5\n",
    "    \n",
    "    # Default to category 3 (Utilities) if no clear category is found\n",
    "    return 3\n",
    "\n",
    "# Define category names for better readability\n",
    "category_names = {\n",
    "    0: \"Models\",\n",
    "    1: \"Views\",\n",
    "    2: \"Controllers\",\n",
    "    3: \"Utilities\",\n",
    "    4: \"Tests\",\n",
    "    5: \"Configuration\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create labels\n",
    "try:\n",
    "    # Create a new column with the extracted labels\n",
    "    labeled_data = data['train'].map(lambda example: {\n",
    "        **example,\n",
    "        'label': extract_file_type(example['path'])\n",
    "    })\n",
    "    \n",
    "    # Count the distribution of labels\n",
    "    label_counts = collections.Counter(labeled_data['label'])\n",
    "    \n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        category_name = category_names.get(label, f\"Unknown-{label}\")\n",
    "        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(label_counts.keys())\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    print(f\"\\nTotal unique labels: {num_labels}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in data preparation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3555b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "try:\n",
    "    # Shuffle the data\n",
    "    shuffled_data = labeled_data.shuffle(seed=42)\n",
    "    \n",
    "    # Split into train (80%), validation (10%), and test (10%)\n",
    "    train_size = int(0.8 * len(shuffled_data))\n",
    "    val_size = int(0.1 * len(shuffled_data))\n",
    "    \n",
    "    train_data = shuffled_data.select(range(train_size))\n",
    "    val_data = shuffled_data.select(range(train_size, train_size + val_size))\n",
    "    test_data = shuffled_data.select(range(train_size + val_size, len(shuffled_data)))\n",
    "    \n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Training set label distribution: {collections.Counter(train_data['label'])}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Validation set label distribution: {collections.Counter(val_data['label'])}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    print(f\"Test set label distribution: {collections.Counter(test_data['label'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instruction-based prompts for the model\n",
    "def create_instruction_prompt(example):\n",
    "    \"\"\"Convert a code example into an instruction-based prompt for language learning.\"\"\"\n",
    "    code = example['content']\n",
    "    label = example['label']\n",
    "    category = category_names.get(label, f\"Unknown-{label}\")\n",
    "    \n",
    "    # Create different types of prompts to help the model learn the language\n",
    "    prompt_types = [\n",
    "        # Explain code functionality\n",
    "        \"Explain what this Swift code does and how it works:\\n\\n\",\n",
    "        \n",
    "        # Identify patterns and features\n",
    "        \"Identify and explain the key Swift language features used in this code:\\n\\n\",\n",
    "        \n",
    "        # Complete or extend code\n",
    "        \"Complete or extend this Swift code with appropriate functionality:\\n\\n\",\n",
    "        \n",
    "        # Fix or improve code\n",
    "        \"Suggest improvements or best practices for this Swift code:\\n\\n\",\n",
    "        \n",
    "        # Understand code structure\n",
    "        f\"This is a Swift {category.lower()} file. Explain its structure and purpose:\\n\\n\",\n",
    "        \n",
    "        # Code generation tasks\n",
    "        \"Write a Swift function that accomplishes the same task as this code but more efficiently:\\n\\n\",\n",
    "        \n",
    "        # Language understanding\n",
    "        \"Explain the Swift syntax and language features demonstrated in this code:\\n\\n\",\n",
    "        \n",
    "        # Learning from examples\n",
    "        \"Study this Swift code example and explain what you can learn from it:\\n\\n\"\n",
    "    ]\n",
    "    \n",
    "    # Select a random prompt type\n",
    "    instruction = random.choice(prompt_types)\n",
    "    \n",
    "    code_section = f\"```swift\\n{code}\\n```\\n\\n\"\n",
    "    \n",
    "    # Create the full prompt\n",
    "    prompt = instruction + code_section\n",
    "    \n",
    "    # Create a detailed response based on the prompt type and code category\n",
    "    if \"Explain what this Swift code does\" in instruction:\n",
    "        response = f\"This Swift code is a {category.lower()} file that \"\n",
    "        if category == \"Models\":\n",
    "            response += \"defines data structures and model objects. \"\n",
    "        elif category == \"Views\":\n",
    "            response += \"implements user interface components. \"\n",
    "        elif category == \"Controllers\":\n",
    "            response += \"manages application logic and coordinates between models and views. \"\n",
    "        elif category == \"Utilities\":\n",
    "            response += \"provides helper functions and extensions. \"\n",
    "        elif category == \"Tests\":\n",
    "            response += \"contains test cases to verify functionality. \"\n",
    "        elif category == \"Configuration\":\n",
    "            response += \"configures application settings and parameters. \"\n",
    "        \n",
    "        response += \"The code uses Swift syntax with \"\n",
    "        \n",
    "        # Add some language-specific details based on code content\n",
    "        if \"class\" in code:\n",
    "            response += \"class definitions, \"\n",
    "        if \"struct\" in code:\n",
    "            response += \"struct definitions, \"\n",
    "        if \"func\" in code:\n",
    "            response += \"function declarations, \"\n",
    "        if \"var\" in code:\n",
    "            response += \"variable declarations, \"\n",
    "        if \"let\" in code:\n",
    "            response += \"constant declarations, \"\n",
    "        if \"guard\" in code or \"if let\" in code:\n",
    "            response += \"optional unwrapping, \"\n",
    "        if \"extension\" in code:\n",
    "            response += \"extensions, \"\n",
    "        if \"protocol\" in code:\n",
    "            response += \"protocol implementations, \"\n",
    "            \n",
    "        # Remove trailing comma and space if present\n",
    "        if response.endswith(\", \"):\n",
    "            response = response[:-2] + \".\"\n",
    "        else:\n",
    "            response += \"various Swift features.\"\n",
    "    \n",
    "    elif \"Identify and explain the key Swift language features\" in instruction:\n",
    "        response = \"This Swift code demonstrates several key language features:\\n\\n\"\n",
    "        \n",
    "        # Add language features based on code content\n",
    "        features = []\n",
    "        if \"class\" in code:\n",
    "            features.append(\"1. **Classes**: Swift classes are reference types that support inheritance and reference counting.\")\n",
    "        if \"struct\" in code:\n",
    "            features.append(\"1. **Structs**: Swift structs are value types that are copied when assigned or passed as arguments.\")\n",
    "        if \"protocol\" in code:\n",
    "            features.append(\"1. **Protocols**: Similar to interfaces in other languages, protocols define a blueprint of methods, properties, and requirements.\")\n",
    "        if \"extension\" in code:\n",
    "            features.append(\"1. **Extensions**: Swift allows adding functionality to existing types through extensions.\")\n",
    "        if \"guard\" in code:\n",
    "            features.append(\"1. **Guard statements**: Used for early returns and unwrapping optionals, improving code readability.\")\n",
    "        if \"if let\" in code or \"guard let\" in code:\n",
    "            features.append(\"1. **Optional binding**: Swift's way of safely unwrapping optional values.\")\n",
    "        if \"enum\" in code:\n",
    "            features.append(\"1. **Enumerations**: Swift enums are first-class types that can have methods and computed properties.\")\n",
    "        if \"func\" in code:\n",
    "            features.append(\"1. **Functions**: Swift functions can have parameters, return values, and support closures.\")\n",
    "        \n",
    "        # If no specific features were identified, add a generic response\n",
    "        if not features:\n",
    "            features.append(\"1. **Swift syntax**: The code demonstrates standard Swift syntax and conventions.\")\n",
    "            features.append(\"2. **Type safety**: Swift's strong type system helps prevent errors at compile time.\")\n",
    "            features.append(\"3. **Readability**: Swift's clean syntax makes code easy to read and maintain.\")\n",
    "        \n",
    "        # Renumber the features\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_parts = feature.split(\": \", 1)\n",
    "            if len(feature_parts) == 2:\n",
    "                features[i] = f\"{i+1}. **{feature_parts[0].split('**')[1]}**: {feature_parts[1]}\"\n",
    "        \n",
    "        response += \"\\n\".join(features)\n",
    "    \n",
    "    elif \"Complete or extend this Swift code\" in instruction or \"Write a Swift function\" in instruction:\n",
    "        # For code generation tasks, provide a thoughtful response about how to approach the task\n",
    "        response = f\"To extend this Swift {category.lower()} code, I would consider the following approach:\\n\\n\"\n",
    "        \n",
    "        if category == \"Models\":\n",
    "            response += \"1. Add additional properties to capture more data attributes\\n\"\n",
    "            response += \"2. Implement Codable protocol for easy JSON serialization\\n\"\n",
    "            response += \"3. Add validation methods to ensure data integrity\\n\"\n",
    "            response += \"4. Include computed properties for derived values\\n\\n\"\n",
    "            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n",
    "            \n",
    "            if \"struct\" in code:\n",
    "                response += \"// Extension to add Codable conformance\\nextension MyStruct: Codable {\\n    // Codable implementation\\n}\\n\\n\"\n",
    "                response += \"// Add validation method\\nextension MyStruct {\\n    func validate() -> Bool {\\n        // Validation logic\\n        return true\\n    }\\n}\\n\"\n",
    "            else:\n",
    "                response += \"// Example extension or additional functionality\\n// that would be appropriate for this model\\n\"\n",
    "            \n",
    "            response += \"```\"\n",
    "            \n",
    "        elif category == \"Views\":\n",
    "            response += \"1. Add UI customization options\\n\"\n",
    "            response += \"2. Implement additional user interaction handlers\\n\"\n",
    "            response += \"3. Add accessibility support\\n\"\n",
    "            response += \"4. Implement view lifecycle methods\\n\\n\"\n",
    "            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n",
    "            response += \"// Example extension or additional functionality\\n// that would be appropriate for this view\\n\"\n",
    "            response += \"```\"\n",
    "            \n",
    "        else:\n",
    "            response += \"1. Add error handling to make the code more robust\\n\"\n",
    "            response += \"2. Implement additional helper methods\\n\"\n",
    "            response += \"3. Add documentation comments to improve code readability\\n\"\n",
    "            response += \"4. Consider performance optimizations where appropriate\\n\\n\"\n",
    "            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n",
    "            response += \"// Example extension or additional functionality\\n// that would be appropriate for this code\\n\"\n",
    "            response += \"```\"\n",
    "    \n",
    "    else:\n",
    "        # Generic response for other prompt types\n",
    "        response = f\"This Swift code demonstrates typical patterns used in {category.lower()} files. \"\n",
    "        response += \"It follows Swift language conventions and showcases proper syntax for defining \"\n",
    "        \n",
    "        if category == \"Models\":\n",
    "            response += \"data structures with properties and methods. Swift models typically use structs for value semantics or classes when reference semantics are needed. The code demonstrates Swift's strong typing system and property declarations.\"\n",
    "        elif category == \"Views\":\n",
    "            response += \"UI components with layout and interaction logic. Swift views often use UIKit or SwiftUI frameworks, with clear separation of UI elements and their behaviors. The code shows how Swift handles user interface components and event responses.\"\n",
    "        elif category == \"Controllers\":\n",
    "            response += \"application logic and coordination between components. Controllers in Swift manage the flow of data between models and views, implementing business logic and handling user interactions. The code demonstrates Swift's approach to application architecture.\"\n",
    "        elif category == \"Utilities\":\n",
    "            response += \"helper functions and extensions to enhance functionality. Swift utilities often leverage the language's powerful extension capabilities to add functionality to existing types. The code shows how Swift can be extended and customized through utility functions.\"\n",
    "        elif category == \"Tests\":\n",
    "            response += \"test cases with setup, execution, and verification steps. Swift tests typically use XCTest framework with arrange-act-assert pattern. The code demonstrates Swift's approach to unit testing and verification.\"\n",
    "        elif category == \"Configuration\":\n",
    "            response += \"application settings and configuration parameters. Swift configuration files often define constants, environment settings, and application parameters. The code shows how Swift handles application configuration and settings management.\"\n",
    "    \n",
    "    # Combine prompt and response for instruction tuning\n",
    "    full_text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{response}\\n\"\n",
    "    \n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"label\": label,\n",
    "        \"category\": category\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create instruction-based datasets\n",
    "try:\n",
    "    # Create instruction datasets\n",
    "    train_instructions = train_data.map(create_instruction_prompt)\n",
    "    val_instructions = val_data.map(create_instruction_prompt)\n",
    "    test_instructions = test_data.map(create_instruction_prompt)\n",
    "    \n",
    "    # Print an example to verify\n",
    "    print(\"Example instruction prompt:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(train_instructions[0]['text'])\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"Created {len(train_instructions)} training instructions\")\n",
    "    print(f\"Created {len(val_instructions)} validation instructions\")\n",
    "    print(f\"Created {len(test_instructions)} test instructions\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating instruction prompts: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84552fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Tokenize the instruction data with proper handling of padding and truncation\n",
    "def tokenize_instruction(examples):\n",
    "    \"\"\"Tokenize the instruction text with explicit padding and truncation settings.\"\"\"\n",
    "    # Process one example at a time to avoid dimension issues\n",
    "    results = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    \n",
    "    for text in examples['text']:\n",
    "        # Tokenize with explicit padding and truncation settings\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=None  # Return Python lists, not PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Add to results\n",
    "        results[\"input_ids\"].append(encoded[\"input_ids\"])\n",
    "        results[\"attention_mask\"].append(encoded[\"attention_mask\"])\n",
    "        results[\"labels\"].append(encoded[\"input_ids\"].copy())  # Copy input_ids for labels\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0610909",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Apply tokenization to each split\n",
    "    tokenized_train = train_instructions.map(\n",
    "        tokenize_instruction,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content', 'label', 'text', 'prompt', 'response', 'category']\n",
    "    )\n",
    "    \n",
    "    tokenized_val = val_instructions.map(\n",
    "        tokenize_instruction,\n",
    "        batched=True,\n",
    "        remove_columns=['repo_name', 'path', 'content', 'label', 'text', 'prompt', 'response', 'category']\n",
    "    )\n",
    "    \n",
    "    # Set the format for PyTorch\n",
    "    tokenized_train.set_format(\"torch\")\n",
    "    tokenized_val.set_format(\"torch\")\n",
    "    \n",
    "    print(f\"Tokenized {len(tokenized_train)} training examples\")\n",
    "    print(f\"Tokenized {len(tokenized_val)} validation examples\")\n",
    "    print(\"Data tokenization complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for training without unnecessary dependencies\n",
    "\n",
    "# Set up training arguments with optimized settings for multi-GPU training\n",
    "try:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Configure training arguments with enhanced memory optimizations for multi-GPU training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        \n",
    "        # Memory optimization settings\n",
    "        fp16=True,                    # Use mixed precision training\n",
    "        bf16=False,                   # Don't use bfloat16 (T4 GPUs don't support it)\n",
    "        gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "        optim=\"adamw_torch_fused\",    # Use memory-efficient fused optimizer\n",
    "        \n",
    "        # Advanced memory settings\n",
    "        max_grad_norm=0.3,            # Reduce gradient norm for stability\n",
    "        group_by_length=True,         # Group sequences of similar length to reduce padding\n",
    "        dataloader_pin_memory=False,  # Disable pinning memory for less RAM usage\n",
    "        \n",
    "        # Distributed training parameters\n",
    "        local_rank=int(os.environ.get(\"LOCAL_RANK\", -1)),  # For distributed training\n",
    "        ddp_find_unused_parameters=True,                   # Required for Phi-3 models in multi-GPU setup\n",
    "        ddp_bucket_cap_mb=50,                             # Limit communication buffer size\n",
    "        dataloader_num_workers=1,                          # Reduced from 4 to save memory\n",
    "        dataloader_prefetch_factor=2,                      # Limit prefetching to save memory\n",
    "        report_to=\"none\",                                  # Disable reporting to avoid overhead\n",
    "    )\n",
    "    \n",
    "    print(f\"Training arguments configured for {'multi-GPU' if torch.cuda.device_count() > 1 else 'single-GPU'} training\")\n",
    "    print(f\"Using gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
    "    print(f\"Using mixed precision: {training_args.fp16}\")\n",
    "    print(f\"Local rank: {training_args.local_rank}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error setting up training arguments: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d18707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Create a custom data collator that properly handles the data\n",
    "class CustomDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features):\n",
    "        # Ensure all features have the same keys\n",
    "        if not all(k in features[0] for k in [\"input_ids\", \"attention_mask\", \"labels\"]):\n",
    "            raise ValueError(\"Some features are missing required keys\")\n",
    "        \n",
    "        # Create a batch with proper padding\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "            \"labels\": torch.stack([f[\"labels\"] for f in features])\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Create data collator for language modeling\n",
    "data_collator = CustomDataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal language modeling, not masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c28e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 3: MODEL INITIALIZATION - Load and quantize the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: MODEL INITIALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"ü§ñ Initializing model with quantization...\")\n",
    "\n",
    "# Create a flag to track which quantization method we're using\n",
    "USING_GGUF = False\n",
    "QUANT_BITS = 2  # Default to 2-bit quantization\n",
    "\n",
    "print(f\"üì• Loading {MODEL_NAME} with {QUANT_BITS}-bit quantization...\")\n",
    "\n",
    "try:\n",
    "    # First check if GGUF libraries are available\n",
    "    if ('ctransformers' in globals() or 'ctransformers' in locals()) and ('llama_cpp' in globals() or 'llama_cpp' in locals()):\n",
    "        # Use GGUF for 2-bit quantization\n",
    "        print(f\"Using GGUF for {QUANT_BITS}-bit quantization...\")\n",
    "        \n",
    "        # First load the model with HF transformers\n",
    "        print(f\"Loading base model {MODEL_NAME} with standard transformers to prepare for GGUF conversion...\")\n",
    "        \n",
    "        # Standard loading with memory optimization\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True,\n",
    "            use_cache=False,  # Disable KV cache for better memory efficiency\n",
    "            low_cpu_mem_usage=True,  # Reduce CPU memory usage during loading\n",
    "        )\n",
    "        \n",
    "        # Create temporary directory for GGUF conversion\n",
    "        import tempfile\n",
    "        import os\n",
    "        from pathlib import Path\n",
    "        \n",
    "        temp_dir = Path(tempfile.mkdtemp())\n",
    "        hf_model_path = temp_dir / \"hf_model\"\n",
    "        gguf_output_path = temp_dir / \"model.gguf\"\n",
    "        \n",
    "        print(f\"Saving model to temporary directory for GGUF conversion: {hf_model_path}\")\n",
    "        \n",
    "        # Save the model in HF format first\n",
    "        model.save_pretrained(hf_model_path)\n",
    "        tokenizer.save_pretrained(hf_model_path)\n",
    "        \n",
    "        # Now apply GGUF 2-bit quantization using conversion tools\n",
    "        print(f\"Converting to GGUF with {QUANT_BITS}-bit quantization...\")\n",
    "        \n",
    "        try:\n",
    "            # First method: Use ctransformers for quantization\n",
    "            from ctransformers.lib import convert_hf_to_gguf\n",
    "            \n",
    "            # Define GGUF quantization type based on bit level\n",
    "            if QUANT_BITS == 2:\n",
    "                quant_type = \"q2_k\"  # 2-bit quantization with k-quants\n",
    "            elif QUANT_BITS == 3:\n",
    "                quant_type = \"q3_k\"  # 3-bit quantization\n",
    "            elif QUANT_BITS == 4:\n",
    "                quant_type = \"q4_k\"  # 4-bit quantization\n",
    "            else:\n",
    "                quant_type = \"q2_k\"  # Default to 2-bit if not specified\n",
    "                \n",
    "            print(f\"Using GGUF quantization type: {quant_type}\")\n",
    "            \n",
    "            # Convert the model to GGUF format with the specified quantization\n",
    "            convert_hf_to_gguf(\n",
    "                str(hf_model_path),\n",
    "                str(gguf_output_path),\n",
    "                quantization_type=quant_type\n",
    "            )\n",
    "            \n",
    "            # Load the GGUF model for verification\n",
    "            from ctransformers import AutoModelForCausalLM as CTAutoModelForCausalLM\n",
    "            \n",
    "            gguf_model = CTAutoModelForCausalLM.from_pretrained(\n",
    "                str(gguf_output_path),\n",
    "                model_type=\"phi\",  # Specify it's a Phi model\n",
    "                gpu_layers=24,     # Use GPU for most layers\n",
    "                context_length=MAX_LENGTH\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully converted and loaded GGUF {QUANT_BITS}-bit quantized model\")\n",
    "            \n",
    "            # Now we need to integrate the GGUF model with our HF training pipeline\n",
    "            # We'll keep the original model for training with LoRA but save the GGUF model for inference\n",
    "            print(\"Saving the GGUF quantized model for inference after training\")\n",
    "            \n",
    "            # Create directory for the GGUF model\n",
    "            os.makedirs(\"./phi3_swift_model_gguf\", exist_ok=True)\n",
    "            \n",
    "            # Copy the GGUF model to the output directory\n",
    "            import shutil\n",
    "            shutil.copy(gguf_output_path, \"./phi3_swift_model_gguf/model.gguf\")\n",
    "            \n",
    "            # Continue with HF model for training but set the flag to indicate we'll use GGUF for inference\n",
    "            USING_GGUF = True\n",
    "            print(f\"Will use GGUF {QUANT_BITS}-bit quantization for inference after training\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with ctransformers GGUF conversion: {e}\")\n",
    "            print(\"Trying alternative GGUF conversion method...\")\n",
    "            \n",
    "            # Try alternative method: Use llama.cpp for conversion if available\n",
    "            try:\n",
    "                import llama_cpp\n",
    "                from llama_cpp import Llama\n",
    "                \n",
    "                # Determine the appropriate quantization type\n",
    "                if QUANT_BITS == 2:\n",
    "                    quant_type = \"Q2_K\"\n",
    "                elif QUANT_BITS == 3:\n",
    "                    quant_type = \"Q3_K\" \n",
    "                elif QUANT_BITS == 4:\n",
    "                    quant_type = \"Q4_K\"\n",
    "                else:\n",
    "                    quant_type = \"Q2_K\"  # Default to 2-bit\n",
    "                \n",
    "                # First check if llama-cpp-python includes the conversion tool directly\n",
    "                if hasattr(llama_cpp, \"convert_hf_to_gguf\"):\n",
    "                    print(\"Using llama-cpp-python's built-in converter\")\n",
    "                    llama_cpp.convert_hf_to_gguf(\n",
    "                        str(hf_model_path),\n",
    "                        str(gguf_output_path),\n",
    "                        quantization_type=quant_type\n",
    "                    )\n",
    "                else:\n",
    "                    # Otherwise, use the command-line tool\n",
    "                    print(\"Using command-line llama-cpp-python converter\")\n",
    "                    import subprocess\n",
    "                    subprocess.check_call([\n",
    "                        \"python\", \"-m\", \"llama_cpp.convert_hf_to_gguf\",\n",
    "                        str(hf_model_path),\n",
    "                        \"--outfile\", str(gguf_output_path),\n",
    "                        \"--quantize\", quant_type\n",
    "                    ])\n",
    "                \n",
    "                # Verify the model can be loaded\n",
    "                model_gguf = Llama(\n",
    "                    model_path=str(gguf_output_path),\n",
    "                    n_ctx=MAX_LENGTH,\n",
    "                    n_gpu_layers=24\n",
    "                )\n",
    "                \n",
    "                print(f\"Successfully converted and loaded GGUF {QUANT_BITS}-bit quantized model with llama-cpp\")\n",
    "                \n",
    "                # Save the model for later use\n",
    "                os.makedirs(\"./phi3_swift_model_gguf\", exist_ok=True)\n",
    "                shutil.copy(gguf_output_path, \"./phi3_swift_model_gguf/model.gguf\")\n",
    "                \n",
    "                USING_GGUF = True\n",
    "                print(f\"Will use GGUF {QUANT_BITS}-bit quantization for inference after training\")\n",
    "                \n",
    "            except Exception as llama_cpp_error:\n",
    "                print(f\"llama-cpp-python GGUF conversion also failed: {llama_cpp_error}\")\n",
    "                raise  # Let it fall through to BitsAndBytes fallback\n",
    "    else:\n",
    "        raise ImportError(\"GGUF libraries not available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    # Fallback to using BitsAndBytes for 4-bit quantization\n",
    "    print(f\"Falling back to BitsAndBytes 4-bit quantization: {e}\")\n",
    "    QUANT_BITS = 4\n",
    "    USING_GGUF = False\n",
    "    \n",
    "    # BitsAndBytes 4-bit quantization with CPU offloading for training\n",
    "    # Note: This requires the multi-backend version of BitsAndBytes we installed earlier\n",
    "    print(\"Using 4-bit quantization with CPU offloading support for training\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        # Add CPU offloading settings for training\n",
    "        bnb_4bit_compute_dtype_for_cpu_offload=torch.float32,\n",
    "        # Additional settings to help with CPU offloaded training\n",
    "        llm_int8_enable_fp32_cpu_offload=True,  # Critical for CPU offloading during training\n",
    "        llm_int8_has_fp16_weight=True,  # Help with mixed precision during offloading\n",
    "        llm_int8_threshold=6.0  # Adjust threshold for better quantization quality\n",
    "    )\n",
    "    \n",
    "    # We'll use hybrid GPU/CPU configuration with multi-backend BitsAndBytes support\n",
    "    print(\"Initializing with CPU offloading for multi-backend BitsAndBytes training...\")\n",
    "    \n",
    "    # First initialize with empty weights to avoid OOM during loading\n",
    "    with init_empty_weights():\n",
    "        config = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME, \n",
    "            trust_remote_code=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        # Add memory-efficient attention method to config if available\n",
    "        if USE_MEMORY_EFFICIENT_ATTENTION and hasattr(config, \"attention_implementation\"):\n",
    "            print(\"Enabling memory-efficient attention...\")\n",
    "            config.attention_implementation = \"flash_attention_2\"\n",
    "    \n",
    "    # Create hybrid device map that allows CPU offloading with multi-backend BitsAndBytes\n",
    "    offload_folder = OFFLOAD_FOLDER if USE_SEQUENTIAL_OFFLOAD else None\n",
    "    print(f\"Using offload folder: {offload_folder}\")\n",
    "    \n",
    "    # Configure device map for multi-backend BitsAndBytes training\n",
    "    device_map = {\n",
    "        \"model.embed_tokens\": 0,  # Keep embeddings on GPU 0\n",
    "        \"model.norm\": 0,          # Keep normalization on GPU 0\n",
    "        \"lm_head\": 0,             # Keep LM head on GPU 0\n",
    "    }\n",
    "    \n",
    "    # Set additional environment variables specific to bitsandbytes multi-backend training\n",
    "    os.environ[\"BNB_ENABLE_TRAINING_OFFLOAD\"] = \"1\"  # Critical for bitsandbytes multi-backend support\n",
    "        \n",
    "        # Distribute layers across devices with CPU/disk offloading\n",
    "        # Get number of layers with compatibility for different model types\n",
    "        n_layers = None\n",
    "        \n",
    "        # First, try direct model name detection which is most reliable for Phi-3 models\n",
    "        if \"phi-3\" in MODEL_NAME.lower() or \"phi3\" in MODEL_NAME.lower():\n",
    "            # Use hardcoded values for known Phi-3 variants\n",
    "            if \"mini\" in MODEL_NAME.lower():\n",
    "                n_layers = 32  # Phi-3-mini has 32 layers\n",
    "                print(f\"Direct model name detection: Phi-3-mini has {n_layers} layers\")\n",
    "            elif \"medium\" in MODEL_NAME.lower():\n",
    "                n_layers = 60  # Phi-3-medium has 60 layers\n",
    "                print(f\"Direct model name detection: Phi-3-medium has {n_layers} layers\")\n",
    "            elif \"small\" in MODEL_NAME.lower():\n",
    "                n_layers = 26  # Phi-3-small has 26 layers\n",
    "                print(f\"Direct model name detection: Phi-3-small has {n_layers} layers\") \n",
    "            else:\n",
    "                n_layers = 32\n",
    "                print(f\"Direct model name detection: Unknown Phi-3 variant, assuming {n_layers} layers\")\n",
    "        \n",
    "        # If we couldn't determine layers by name, try config attributes\n",
    "        if n_layers is None:\n",
    "            # Try different attribute names used by various model architectures\n",
    "            for attr_name in [\"num_hidden_layers\", \"n_layer\", \"num_layers\", \"n_blocks\"]:\n",
    "                if hasattr(config, attr_name):\n",
    "                    n_layers = getattr(config, attr_name)\n",
    "                    print(f\"Found layers count using config.{attr_name}: {n_layers}\")\n",
    "                    break\n",
    "        \n",
    "        # As fallback for Phi-3 models, try to detect layers by architecture patterns\n",
    "        if n_layers is None:\n",
    "            # For Phi-3 models specifically\n",
    "            if hasattr(config, \"model_type\") and \"phi\" in config.model_type.lower():\n",
    "                # Phi-3 typically has 32 layers in the mini version\n",
    "                if \"mini\" in MODEL_NAME.lower():\n",
    "                    n_layers = 32\n",
    "                # Phi-3-medium typically has 60 layers\n",
    "                elif \"medium\" in MODEL_NAME.lower():\n",
    "                    n_layers = 60\n",
    "                # Phi-3-small typically has 26 layers\n",
    "                elif \"small\" in MODEL_NAME.lower():\n",
    "                    n_layers = 26\n",
    "                else:\n",
    "                    # Default to 32 for unknown Phi-3 variants\n",
    "                    n_layers = 32\n",
    "                print(f\"Using estimated layers for Phi-3 model: {n_layers}\")\n",
    "            else:\n",
    "                # Last resort default\n",
    "                n_layers = 24\n",
    "                print(f\"Warning: Could not determine number of layers, using default: {n_layers}\")\n",
    "        \n",
    "        # With multi-backend BitsAndBytes, we can use CPU offloading for training\n",
    "        # Split layers across GPUs and CPU optimally\n",
    "        gpu0_layers = n_layers // 3      # ~33% on GPU 0\n",
    "        gpu1_layers = n_layers // 3      # ~33% on GPU 1\n",
    "        cpu_layers = n_layers - gpu0_layers - gpu1_layers  # ~34% on CPU with offloading\n",
    "        print(f\"Using hybrid configuration with multi-backend BitsAndBytes: {gpu0_layers} layers on GPU 0, {gpu1_layers} layers on GPU 1, {cpu_layers} layers on CPU\")\n",
    "        \n",
    "        # Determine the correct layer naming pattern for different model architectures\n",
    "        # For Phi-3 models, the pattern might be different than standard transformers\n",
    "        layer_prefix = \"model.layers\"  # Default pattern\n",
    "        \n",
    "        # Try to determine correct layer prefix from model config\n",
    "        if hasattr(config, \"model_type\"):\n",
    "            model_type = config.model_type.lower()\n",
    "            if \"phi\" in model_type:\n",
    "                # Phi models may use different naming conventions\n",
    "                # We'll try a few common patterns for Phi-3\n",
    "                layer_prefix_options = [\n",
    "                    \"model.layers\",         # Standard pattern\n",
    "                    \"transformer.h\",        # Some Phi models use this\n",
    "                    \"model.decoder.layers\", # Another common pattern\n",
    "                    \"model.transformer.h\",  # Yet another pattern\n",
    "                ]\n",
    "                \n",
    "                # Log the possible patterns we're going to try\n",
    "                print(f\"Phi model detected, will try these layer prefix patterns: {layer_prefix_options}\")\n",
    "                \n",
    "                # Use the first one by default, the device_map will be adjusted at runtime if needed\n",
    "                layer_prefix = layer_prefix_options[0]\n",
    "        \n",
    "        print(f\"Using layer prefix pattern: {layer_prefix}\")\n",
    "        \n",
    "        # Assign layers to devices with multiple pattern fallbacks\n",
    "        for i in range(n_layers):\n",
    "            layer_device = None\n",
    "            if i < gpu0_layers:\n",
    "                layer_device = 0\n",
    "            elif i < gpu0_layers + gpu1_layers:\n",
    "                layer_device = 1\n",
    "            else:\n",
    "                layer_device = \"cpu\"\n",
    "            \n",
    "            # Add all possible layer naming patterns to device map to ensure we catch the right one\n",
    "            if hasattr(config, \"model_type\") and \"phi\" in config.model_type.lower():\n",
    "                # Add all possible patterns for Phi models - optimized for Phi-3\n",
    "                # Primary pattern for Phi-3\n",
    "                device_map[f\"model.layers.{i}\"] = layer_device\n",
    "                \n",
    "                # Additional patterns with Phi-3-specific paths\n",
    "                device_map[f\"transformer.h.{i}\"] = layer_device\n",
    "                device_map[f\"model.decoder.layers.{i}\"] = layer_device\n",
    "                device_map[f\"model.transformer.h.{i}\"] = layer_device\n",
    "                \n",
    "                # Phi-3-specific naming patterns based on model inspection\n",
    "                device_map[f\"phi_model.layers.{i}\"] = layer_device\n",
    "                device_map[f\"layers.{i}\"] = layer_device\n",
    "                \n",
    "                # For absolute reliability, add all possible nested paths where Phi-3 layers could be\n",
    "                device_map[f\"base_model.model.layers.{i}\"] = layer_device\n",
    "                device_map[f\"model.model.layers.{i}\"] = layer_device\n",
    "            else:\n",
    "                # Standard pattern for other models\n",
    "                device_map[f\"{layer_prefix}.{i}\"] = layer_device\n",
    "        \n",
    "        print(f\"Device map: GPU 0: {gpu0_layers} layers, GPU 1: {gpu1_layers} layers, CPU: {cpu_layers} layers\")\n",
    "        \n",
    "        # Load with offloading and quantization - with error handling and fallbacks\n",
    "        try:\n",
    "            print(\"Attempting to load model with custom device map and CPU offloading...\")\n",
    "            \n",
    "            # Create a modified version of BnB config that enables CPU offloading\n",
    "            # This is required when using quantization with CPU offloading \n",
    "            cpu_offload_bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                llm_int8_enable_fp32_cpu_offload=True,  # Enable CPU offloading with quantized models\n",
    "                llm_int8_threshold=6.0,  # Increase threshold for more efficient offloading\n",
    "            )\n",
    "            \n",
    "            # Kaggle-optimized max memory allocation - be more conservative\n",
    "            max_memory = {\n",
    "                0: \"9GB\",          # Reserve more headroom on GPU 0\n",
    "                1: \"9GB\",          # Reserve more headroom on GPU 1\n",
    "                \"cpu\": \"24GB\",     # Limit CPU memory usage for Kaggle\n",
    "            }\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                quantization_config=cpu_offload_bnb_config,  # Use modified config with CPU offloading enabled\n",
    "                device_map=device_map,\n",
    "                offload_folder=offload_folder,\n",
    "                offload_state_dict=True,\n",
    "                max_memory=max_memory,  # Add explicit memory limits\n",
    "                low_cpu_mem_usage=True,  # Enable more aggressive CPU memory optimization\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True,\n",
    "                use_cache=False,\n",
    "                attn_implementation=\"eager\"  # Always use eager implementation for compatibility\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading with custom device map: {e}\")\n",
    "            print(\"Falling back to automatic device map...\")\n",
    "            \n",
    "            # Perform deep cleanup for Kaggle environment\n",
    "            for _ in range(3):  # Multiple cleanup passes\n",
    "                cleanup_memory()  \n",
    "                time.sleep(1)  # Give the system time to reclaim memory\n",
    "            \n",
    "            # Try again with much simpler configuration optimized for Kaggle\n",
    "            print(\"Using simplified auto device mapping for maximum compatibility...\")\n",
    "            \n",
    "            # Create simpler BnB config with multi-backend support for auto device mapping\n",
    "            simple_bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                # Keep multi-backend support even in fallback mode\n",
    "                llm_int8_enable_fp32_cpu_offload=True,\n",
    "                llm_int8_has_fp16_weight=True\n",
    "            )\n",
    "            \n",
    "            # Set multi-backend environment variables for fallback as well\n",
    "            os.environ[\"BNB_ENABLE_TRAINING_OFFLOAD\"] = \"1\"\n",
    "            \n",
    "            # Kaggle-optimized max memory settings - be very conservative\n",
    "            kaggle_max_memory = {\n",
    "                0: \"8GB\",          # Very conservative GPU 0 limit for Kaggle \n",
    "                1: \"8GB\",          # Very conservative GPU 1 limit for Kaggle\n",
    "                \"cpu\": \"24GB\",     # Limited CPU memory for Kaggle\n",
    "            }\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                quantization_config=simple_bnb_config,\n",
    "                device_map=\"auto\",  # Let HF decide automatically - most compatible option\n",
    "                max_memory=kaggle_max_memory,  # Conservative memory limits for Kaggle\n",
    "                low_cpu_mem_usage=True,  # More aggressive CPU memory optimization\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True,\n",
    "                use_cache=False\n",
    "            )\n",
    "    else:\n",
    "        # Standard loading with basic memory optimization\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "    print(\"Successfully loaded model with BitsAndBytes 4-bit quantization\")\n",
    "\n",
    "# Configure LoRA for fine-tuning with memory optimizations\n",
    "print(\"Setting up memory-optimized LoRA fine-tuning...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target only essential modules to save memory\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    modules_to_save=None,  # Don't save any modules fully to save memory\n",
    ")\n",
    "\n",
    "# Prepare the model for training with LoRA and additional memory optimizations\n",
    "print(\"Preparing model for k-bit training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Enable activation checkpointing to save memory if requested\n",
    "if USE_ACTIVATION_CHECKPOINTING:\n",
    "    print(\"Enabling activation checkpointing to save memory...\")\n",
    "    try:\n",
    "        # For transformers models\n",
    "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "            model.gradient_checkpointing_enable()\n",
    "            print(\"Gradient checkpointing enabled via model method\")\n",
    "        # For torch models\n",
    "        elif hasattr(model, \"enable_input_require_grads\"):\n",
    "            model.enable_input_require_grads()\n",
    "            print(\"Input require grads enabled\")\n",
    "            \n",
    "        # Enable checkpointing on specific modules (for newer transformers versions)\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, \"_use_gradient_checkpointing\"):\n",
    "                module._use_gradient_checkpointing = True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not enable activation checkpointing: {e}\")\n",
    "\n",
    "# Apply LoRA and free memory\n",
    "print(\"Applying LoRA adapter...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Run explicit memory cleanup after model initialization\n",
    "cleanup_memory()\n",
    "\n",
    "# Report model parameter counts\n",
    "print(f\"Model trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "print(f\"Model parameters using memory: {sum(p.numel() * (2 if p.dtype == torch.float16 else 4) for p in model.parameters()) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Print information about the quantized model\n",
    "quant_method = \"GGUF\" if USING_GGUF else \"BitsAndBytes\"\n",
    "print(f\"‚úÖ Model loaded and configured with {QUANT_BITS}-bit {quant_method} quantization and LoRA (rank={LORA_R})\")\n",
    "print(f\"Model architecture: {model.__class__.__name__}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n",
    "\n",
    "# Update execution status\n",
    "if 'update_status' in globals():\n",
    "    update_status(\"model_initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312558c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# SECTION 4: TRAINER SETUP - Configure the training parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4: TRAINER SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"üîß Creating trainer and configuring training parameters...\")\n",
    "\n",
    "# Create trainer with memory optimizations for GPU/CPU\n",
    "print(\"Setting up memory-optimized trainer...\")\n",
    "\n",
    "# Ensure environment variables are set for transformers\n",
    "os.environ[\"TRANSFORMERS_SKIP_TORCH_VISION_IMPORT\"] = \"1\"\n",
    "\n",
    "# Add additional memory optimization callbacks\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "class OOMGuardCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback that detects and prevents OOM errors\"\"\"\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Check memory usage on each GPU after step\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / torch.cuda.get_device_properties(i).total_memory\n",
    "            if allocated > 0.92:  # Over 92% memory usage\n",
    "                # Force immediate garbage collection and cache clearing\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"\\n‚ö†Ô∏è Warning: GPU {i} memory usage high ({allocated*100:.1f}%). Forcing cache clear.\")\n",
    "                # Pause briefly to allow memory release\n",
    "                time.sleep(1)\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Clean up between epochs for better stability\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return control\n",
    "\n",
    "class MemoryOptimizationCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to optimize memory usage during training.\"\"\"\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Run cleanup after each optimization step.\"\"\"\n",
    "        # Free memory every few steps\n",
    "        if state.global_step % 10 == 0:\n",
    "            cleanup_memory()\n",
    "            \n",
    "            # Monitor GPU memory usage every 100 steps\n",
    "            if state.global_step % 100 == 0:\n",
    "                print(f\"\\n--- Memory status at step {state.global_step} ---\")\n",
    "                if torch.cuda.is_available():\n",
    "                    for i in range(torch.cuda.device_count()):\n",
    "                        allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "                        reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "                        print(f\"GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "                \n",
    "                # Also monitor CPU memory\n",
    "                process = psutil.Process(os.getpid())\n",
    "                memory_info = process.memory_info()\n",
    "                print(f\"CPU memory: {memory_info.rss / (1024**3):.2f}GB\")\n",
    "        \n",
    "        return control\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        \"\"\"Run before evaluation begins.\"\"\"\n",
    "        print(\"\\nRunning memory cleanup before evaluation...\")\n",
    "        cleanup_memory()\n",
    "        return control\n",
    "    \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"Run before model is saved.\"\"\"\n",
    "        print(\"\\nRunning memory cleanup before saving model...\")\n",
    "        cleanup_memory()\n",
    "        return control\n",
    "        \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Run at the end of each epoch.\"\"\"\n",
    "        print(f\"\\nCompleted epoch. Running thorough memory cleanup...\")\n",
    "        cleanup_memory()\n",
    "        return control\n",
    "\n",
    "# Create custom memory-efficient trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        early_stopping_callback,\n",
    "        MemoryOptimizationCallback(),\n",
    "        OOMGuardCallback()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Verify model device\n",
    "model_device = next(model.parameters()).device\n",
    "device_type = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Model is on {device_type} device: {model_device}\")\n",
    "\n",
    "print(\"‚úÖ Trainer setup complete\")\n",
    "\n",
    "# Update execution status\n",
    "if 'update_status' in globals():\n",
    "    update_status(\"trainer_created\")\n",
    "    \n",
    "print(\"Ready to start training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f90e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 5: TRAINING - Train the model on the dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5: TRAINING PROCESS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Function to monitor system resources during training\n",
    "def monitor_resources():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    mem = psutil.virtual_memory()\n",
    "    cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "    \n",
    "    print(f\"\\nSystem Resources:\")\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")\n",
    "    print(f\"Process Memory: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"System Memory: {mem.percent}% used, {mem.available / 1024 / 1024:.2f} MB available\")\n",
    "    \n",
    "    # Show memory metrics for the active device\n",
    "    if torch.cuda.is_available():\n",
    "        # GPU memory monitoring\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nGPU Memory Usage ({num_gpus} GPUs detected):\")\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            properties = torch.cuda.get_device_properties(i)\n",
    "            total_memory = properties.total_memory / (1024**3)\n",
    "            free_memory = total_memory - allocated\n",
    "            \n",
    "            print(f\"  GPU {i} ({properties.name}):\")\n",
    "            print(f\"    Total memory: {total_memory:.2f} GB\")\n",
    "            print(f\"    Allocated: {allocated:.2f} GB ({allocated/total_memory*100:.1f}%)\")\n",
    "            print(f\"    Free: {free_memory:.2f} GB ({free_memory/total_memory*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\")  # Add blank line for readability\n",
    "\n",
    "\n",
    "print(\"üöÄ Starting training process with enhanced memory management...\")\n",
    "print(\"This will take some time. Training progress will be displayed below.\")\n",
    "\n",
    "# Run training with aggressive memory optimization for multi-GPU setup\n",
    "try:\n",
    "    # Monitor resources before training\n",
    "    print(\"Resources before training:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Super aggressive memory cleanup before training\n",
    "    print(\"Performing deep memory cleanup before training...\")\n",
    "    # Force multiple garbage collection passes\n",
    "    for _ in range(3):\n",
    "        cleanup_memory()\n",
    "        time.sleep(0.5)  # Brief pause to allow OS to reclaim memory\n",
    "    \n",
    "    # Set environment variables for even more aggressive memory management\n",
    "    os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"  # Disable CUDA caching\n",
    "    \n",
    "    # Configure PyTorch for memory-efficient multi-GPU training\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Configuring PyTorch for memory-efficient multi-GPU training...\")\n",
    "        # Enable TF32 precision for faster training (on Ampere GPUs)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        \n",
    "        # More conservative memory allocation strategy\n",
    "        torch.cuda.set_per_process_memory_fraction(0.85)  # Reserve more memory (15%) for system\n",
    "        \n",
    "        # For Kaggle, use the most reliable attention mechanism rather than Flash Attention\n",
    "        print(\"Using standard attention for maximum Kaggle compatibility\")\n",
    "            \n",
    "        # Configure CUDA memory usage more conservatively for Kaggle\n",
    "        if torch.cuda.is_available():\n",
    "            # Set more conservative memory fraction for Kaggle's T4 GPUs\n",
    "            torch.cuda.set_per_process_memory_fraction(0.8)  # Reserve 20% for Kaggle system\n",
    "                \n",
    "            # For Kaggle's T4 GPUs, avoid enabling TF32 which can consume more memory\n",
    "            torch.backends.cuda.matmul.allow_tf32 = False\n",
    "            torch.backends.cudnn.allow_tf32 = False\n",
    "                \n",
    "            # Explicitly disable any caching mechanisms that could leak memory\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "                \n",
    "            print(\"Configured CUDA memory settings for Kaggle environment\")\n",
    "                \n",
    "        # Set additional environment variables specifically for Kaggle\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable tokenizer parallelism to save memory\n",
    "    \n",
    "    # Set up a memory monitor thread for continuous monitoring during training\n",
    "    def memory_monitoring_thread():\n",
    "        print(\"Starting memory monitoring thread...\")\n",
    "        while True:\n",
    "            try:\n",
    "                # Only log if we're close to OOM\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "                    total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "                    if allocated / total > 0.85:  # If using more than 85% of memory\n",
    "                        print(f\"‚ö†Ô∏è WARNING: GPU {i} memory usage high: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)\")\n",
    "                        cleanup_memory()  # Try to free up memory\n",
    "            except Exception as e:\n",
    "                print(f\"Error in memory monitoring: {e}\")\n",
    "            time.sleep(10)  # Check every 10 seconds\n",
    "\n",
    "    # Start memory monitoring in a separate thread if not in debug mode\n",
    "    if not DEBUG_MODE:\n",
    "        import threading\n",
    "        monitor_thread = threading.Thread(target=memory_monitoring_thread, daemon=True)\n",
    "        monitor_thread.start()\n",
    "    \n",
    "    # Start training \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Configure training to catch OOM errors and retry with more aggressive settings\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"\\nüöÄ Starting training (attempt {attempt+1}/{max_retries})...\")\n",
    "            \n",
    "            # Additional cleanup right before training\n",
    "            cleanup_memory()\n",
    "            \n",
    "            # Run training\n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            # If we get here, training succeeded\n",
    "            print(\"‚úÖ Training completed successfully!\")\n",
    "            break\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            # Check if this is an OOM error\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"‚ùå CUDA out of memory error (attempt {attempt+1}/{max_retries})\")\n",
    "                \n",
    "                # More aggressive cleanup\n",
    "                cleanup_memory()\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    # Try more aggressive memory saving for next attempt\n",
    "                    print(\"Applying more aggressive memory optimizations for next attempt...\")\n",
    "                    \n",
    "                    # Reduce batch size if possible\n",
    "                    if trainer.args.per_device_train_batch_size > 1:\n",
    "                        trainer.args.per_device_train_batch_size //= 2\n",
    "                        trainer.args.per_device_eval_batch_size //= 2\n",
    "                        print(f\"Reduced batch size to {trainer.args.per_device_train_batch_size}\")\n",
    "                    \n",
    "                    # Increase gradient accumulation steps\n",
    "                    trainer.args.gradient_accumulation_steps *= 2\n",
    "                    print(f\"Increased gradient accumulation to {trainer.args.gradient_accumulation_steps}\")\n",
    "                    \n",
    "                    # Wait a moment for memory to be fully reclaimed\n",
    "                    time.sleep(10)\n",
    "                else:\n",
    "                    print(\"Maximum retry attempts reached. Training failed.\")\n",
    "                    raise\n",
    "            else:\n",
    "                # This is not an OOM error, re-raise\n",
    "                raise\n",
    "    \n",
    "    # Monitor resources after training\n",
    "    print(\"Resources after training:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Print training results\n",
    "    print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "    # Save the model with appropriate method based on quantization used\n",
    "    print(\"\\nüíæ Saving trained model...\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    \n",
    "    # Determine the quantization method for display\n",
    "    quant_method = \"GGUF\" if USING_GGUF else \"BitsAndBytes\"\n",
    "    print(f\"‚úÖ Model saved to {OUTPUT_DIR} ({QUANT_BITS}-bit {quant_method} quantized)\")\n",
    "    if USING_GGUF:\n",
    "        print(f\"‚úÖ GGUF model also saved to {OUTPUT_DIR}/model.gguf\")\n",
    "    print(f\"   Trained on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    \n",
    "    # Save model configuration details\n",
    "    with open(f\"{OUTPUT_DIR}/quantization_config.json\", \"w\") as f:\n",
    "        config_data = {\n",
    "            \"quantization_method\": quant_method,\n",
    "            \"bits\": QUANT_BITS,\n",
    "            \"lora_rank\": LORA_R,\n",
    "            \"lora_alpha\": LORA_ALPHA,\n",
    "            \"original_model\": MODEL_NAME,\n",
    "            \"max_length\": MAX_LENGTH,\n",
    "            \"training_dataset\": DATASET_ID,\n",
    "            \"training_date\": time.strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "        json.dump(config_data, f, indent=2)\n",
    "        print(\"‚úÖ Model configuration saved\")\n",
    "    \n",
    "    # Skip creating loading instructions - we just want the model files\n",
    "    \n",
    "    # Add a minimal config file for reference\n",
    "    with open(f\"{OUTPUT_DIR}/model_info.txt\", \"w\") as f:\n",
    "        f.write(f\"\"\"MODEL INFO\n",
    "Model: {MODEL_NAME}\n",
    "Quantization: {quant_method} {QUANT_BITS}-bit\n",
    "Trained on: {DATASET_ID}\n",
    "Date: {time.strftime(\"%Y-%m-%d\")}\n",
    "\"\"\")\n",
    "        print(\"‚úÖ Model info saved\")\n",
    "    \n",
    "    # Update execution status\n",
    "    if 'update_status' in globals():\n",
    "        update_status(\"training_complete\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    print(\"üßπ Cleaning up memory...\")\n",
    "    cleanup_memory()\n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during training: {e}\")\n",
    "    \n",
    "    # Print stack trace for debugging\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Monitor resources after error\n",
    "    print(\"Resources after error:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Update status to indicate failure\n",
    "    if 'update_status' in globals():\n",
    "        EXECUTION_STATUS[\"training_error\"] = True\n",
    "        print(\"Training failed. Please check the error message above.\")\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81935932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 6: TESTING - Evaluate the trained model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 6: MODEL TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    print(f\"üß™ Testing the {QUANT_BITS}-bit {quant_method} quantized model with Swift code examples...\")\n",
    "    print(f\"This will generate responses to evaluate the model's capabilities.\")\n",
    "    \n",
    "    # For testing, we use the model we already have loaded\n",
    "    test_model = model\n",
    "    \n",
    "    # Function to generate responses for test examples\n",
    "    def generate_response(prompt):\n",
    "        print(f\"Generating response for: {prompt.split('<|assistant|>')[0].split('<|user|>')[-1].strip()[:50]}...\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            # Generate with the quantized model\n",
    "            outputs = test_model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract just the assistant's response\n",
    "        if \"<|assistant|>\" in response:\n",
    "            response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "        return response\n",
    "    \n",
    "    # Test prompts for different Swift language tasks\n",
    "    test_prompts = [\n",
    "        # Explain Swift syntax\n",
    "        \"<|user|>\\nExplain the key features of Swift's optional unwrapping syntax:\\n\\n```swift\\nfunc processName(_ name: String?) {\\n    guard let unwrappedName = name else {\\n        print(\\\"No name provided\\\")\\n        return\\n    }\\n    print(\\\"Hello, \\\\(unwrappedName)!\\\")\\n}\\n```\\n<|assistant|>\",\n",
    "        \n",
    "        # Code completion\n",
    "        \"<|user|>\\nComplete this Swift function that calculates the factorial of a number:\\n\\n```swift\\nfunc factorial(_ n: Int) -> Int {\\n    // Add implementation here\\n}\\n```\\n<|assistant|>\",\n",
    "        \n",
    "        # Debugging help\n",
    "        \"<|user|>\\nWhat's wrong with this Swift code and how can I fix it?\\n\\n```swift\\nclass Person {\\n    var name: String\\n    var age: Int\\n    \\n    func greet() {\\n        print(\\\"Hello, my name is \\\\(name) and I am \\\\(age) years old.\\\")\\n    }\\n}\\n\\nlet person = Person()\\nperson.greet()\\n```\\n<|assistant|>\",\n",
    "        \n",
    "        # Swift best practices\n",
    "        \"<|user|>\\nExplain Swift best practices for error handling:\\n<|assistant|>\"\n",
    "    ]\n",
    "    \n",
    "    # Generate and print responses\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\nüìù Test {i+1}/{len(test_prompts)}:\\n{'-'*40}\")\n",
    "        print(f\"Prompt: {prompt.split('<|assistant|>')[0].replace('<|user|>', '')}\")\n",
    "        response = generate_response(prompt)\n",
    "        print(f\"\\nResponse:\\n{response}\\n\")\n",
    "        \n",
    "        # Add a small delay for better readability in logs\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(\"\\n‚úÖ Testing complete! If the responses look good, your model has been trained successfully.\")\n",
    "    print(\"If you're not satisfied with the quality, you might want to train for more epochs or adjust the training parameters.\")\n",
    "    \n",
    "    # Update execution status\n",
    "    if 'update_status' in globals():\n",
    "        update_status(\"testing_complete\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during testing: {e}\")\n",
    "    print(\"Detailed error information:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Update status to indicate testing error\n",
    "    if 'update_status' in globals():\n",
    "        EXECUTION_STATUS[\"testing_error\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 7: EXECUTION SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 7: EXECUTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Print final execution status\n",
    "if 'EXECUTION_STATUS' in globals():\n",
    "    print(\"\\nüìä Execution Status Summary:\")\n",
    "    for stage, status in EXECUTION_STATUS.items():\n",
    "        if 'error' not in stage:  # Skip error flags in the summary view\n",
    "            icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "            print(f\"{icon} {stage.replace('_', ' ').title()}\")\n",
    "    \n",
    "    # Check if we completed successfully\n",
    "    core_stages = ['setup_complete', 'data_loaded', 'model_initialized', \n",
    "                  'trainer_created', 'training_complete', 'testing_complete']\n",
    "    success = all(EXECUTION_STATUS.get(stage, False) for stage in core_stages)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ SUCCESS: Complete training pipeline executed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è INCOMPLETE: Some stages of the pipeline did not complete.\")\n",
    "        # Find the first incomplete stage\n",
    "        for stage in core_stages:\n",
    "            if not EXECUTION_STATUS.get(stage, False):\n",
    "                print(f\"First incomplete stage: {stage.replace('_', ' ').title()}\")\n",
    "                break\n",
    "\n",
    "print(\"\\nüìã Final Summary:\")\n",
    "print(f\"- Model: {MODEL_NAME}\")\n",
    "print(f\"- Quantization: {QUANT_BITS}-bit {quant_method if 'quant_method' in globals() else 'quantization'}\")\n",
    "print(f\"- Dataset: {DATASET_ID}\")\n",
    "print(f\"- Saved model location: ./phi3_swift_model\")\n",
    "print(f\"- Status: {'Successfully trained and tested' if 'success' in locals() and success else 'Incomplete training process'}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"1. Use your trained model for Swift programming tasks\")\n",
    "print(\"2. Deploy the model to your application\")\n",
    "print(\"3. Continue fine-tuning with more data if needed\")\n",
    "print(\"4. Experiment with different quantization settings\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Thank you for using the Phi-3 training pipeline!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
